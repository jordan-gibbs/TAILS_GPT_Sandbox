[
    {
        "id": "2009.05794",
        "title": "bars-ctr: open benchmarking for click-through rate prediction",
        "abstract": "click-through rate (ctr) prediction is a critical task for many applications, as its accuracy has a direct impact on user experience and platform revenue. in recent years, ctr prediction has been widely studied in both academia and industry, resulting in a wide variety of ctr prediction models. unfortunately, there is still a lack of standardized benchmarks and uniform evaluation protocols for ctr prediction research. this leads to non-reproducible or even inconsistent experimental results among existing studies, which largely limits the practical value and potential impact of their research. in this work, we aim to perform open benchmarking for ctr prediction and present a rigorous comparison of different models in a reproducible manner. to this end, we ran over 7,000 experiments for more than 12,000 gpu hours in total to re-evaluate 24 existing models on multiple datasets and settings. surprisingly, our experiments show that with sufficient hyper-parameter search and model tuning, many deep models have smaller differences than expected. the results also reveal that making real progress on the modeling of ctr prediction is indeed a very challenging research task. we believe that our benchmarking work could not only allow researchers to gauge the effectiveness of new models conveniently but also make them fairly compare with the state of the arts. we have publicly released the benchmarking code, evaluation protocols, and hyper-parameter settings of our work to promote reproducible research in this field.",
        "doi": "",
        "created": "2020-09-12",
        "url": "https://arxiv.org/abs/2009.05794",
        "authors": [
            "jieming zhu",
            "jinyang liu",
            "shuai yang",
            "qi zhang",
            "xiuqiang he"
        ]
    },
    {
        "id": "2109.12613",
        "title": "simplex: a simple and strong baseline for collaborative filtering",
        "abstract": "collaborative filtering (cf) is a widely studied research topic in recommender systems. the learning of a cf model generally depends on three major components, namely interaction encoder, loss function, and negative sampling. while many existing studies focus on the design of more powerful interaction encoders, the impacts of loss functions and negative sampling ratios have not yet been well explored. in this work, we show that the choice of loss function as well as negative sampling ratio is equivalently important. more specifically, we propose the cosine contrastive loss (ccl) and further incorporate it to a simple unified cf model, dubbed simplex. extensive experiments have been conducted on 11 benchmark datasets and compared with 29 existing cf models in total. surprisingly, the results show that, under our ccl loss and a large negative sampling ratio, simplex can surpass most sophisticated state-of-the-art models by a large margin (e.g., max 48.5% improvement in ndcg@20 over lightgcn). we believe that simplex could not only serve as a simple strong baseline to foster future research on cf, but also shed light on the potential research direction towards improving loss function and negative sampling. our source code will be available at https://reczoo.github.io/simplex.",
        "doi": "",
        "created": "2021-09-26",
        "url": "https://arxiv.org/abs/2109.12613",
        "authors": [
            "kelong mao",
            "jieming zhu",
            "jinpeng wang",
            "quanyu dai",
            "zhenhua dong",
            "xi xiao",
            "xiuqiang he"
        ]
    },
    {
        "id": "2111.12727",
        "title": "generating more pertinent captions by leveraging semantics and style on   multi-source datasets",
        "abstract": "this paper addresses the task of generating fluent descriptions by training on a non-uniform combination of data sources, containing both human-annotated and web-collected captions. large-scale datasets with noisy image-text pairs, indeed, provide a sub-optimal source of supervision because of their low-quality descriptive style, while human-annotated datasets are cleaner but smaller in scale. to get the best of both worlds, we propose to leverage and separate semantics and descriptive style through the incorporation of a style token and keywords extracted through a retrieval component. the proposed model avoids the need of object detectors, is trained with a single objective of prompt language modeling, and can replicate the style of human-collected captions while training on sources with different input styles. experimentally, the model shows a strong capability of recognizing real-world concepts and producing high-quality captions. extensive experiments are performed on different image captioning datasets, including cc3m, nocaps, and the competitive coco dataset, where our model consistently outperforms baselines and state-of-the-art approaches.",
        "doi": "",
        "created": "2021-11-24",
        "url": "https://arxiv.org/abs/2111.12727",
        "authors": [
            "marcella cornia",
            "lorenzo baraldi",
            "giuseppe fiameni",
            "rita cucchiara"
        ]
    },
    {
        "id": "2210.03628",
        "title": "graspcaps: a capsule network approach for familiar 6dof object grasping",
        "abstract": "as robots become more widely available outside industrial settings, the need for reliable object grasping and manipulation is increasing. in such environments, robots must be able to grasp and manipulate novel objects in various situations. this paper presents graspcaps, a novel architecture based on capsule networks for generating per-point 6d grasp configurations for familiar objects. graspcaps extracts a rich feature vector of the objects present in the point cloud input, which is then used to generate per-point grasp vectors. this approach allows the network to learn specific grasping strategies for each object category. in addition to graspcaps, the paper also presents a method for generating a large object-grasping dataset using simulated annealing. the obtained dataset is then used to train the graspcaps network. through extensive experiments, we evaluate the performance of the proposed approach, particularly in terms of the success rate of grasping familiar objects in challenging real and simulated scenarios. the experimental results showed that the overall object-grasping performance of the proposed approach is significantly better than the selected baseline. this superior performance highlights the effectiveness of the graspcaps in achieving successful object grasping across various scenarios.",
        "doi": "",
        "created": "2022-10-07",
        "url": "https://arxiv.org/abs/2210.03628",
        "authors": [
            "tomas van der velde",
            "hamed ayoobi",
            "hamidreza kasaei"
        ]
    },
    {
        "id": "2210.06186",
        "title": "gotcha: real-time video deepfake detection via challenge-response",
        "abstract": "with the rise of ai-enabled real-time deepfakes (rtdfs), the integrity of online video interactions has become a growing concern. rtdfs have now made it feasible to replace an imposter's face with their victim in live video interactions. such advancement in deepfakes also coaxes detection to rise to the same standard. however, existing deepfake detection techniques are asynchronous and hence ill-suited for rtdfs. to bridge this gap, we propose a challenge-response approach that establishes authenticity in live settings. we focus on talking-head style video interaction and present a taxonomy of challenges that specifically target inherent limitations of rtdf generation pipelines. we evaluate representative examples from the taxonomy by collecting a unique dataset comprising eight challenges, which consistently and visibly degrades the quality of state-of-the-art deepfake generators. these results are corroborated both by humans and a new automated scoring function, leading to 88.6\\% and 73.2% auc, respectively. the findings underscore the promising potential of challenge-response systems for explainable and scalable real-time deepfake detection in practical scenarios.",
        "doi": "",
        "created": "2022-10-12",
        "url": "https://arxiv.org/abs/2210.06186",
        "authors": [
            "govind mittal",
            "chinmay hegde",
            "nasir memon"
        ]
    },
    {
        "id": "2211.06841",
        "title": "point-dae: denoising autoencoders for self-supervised point cloud   learning",
        "abstract": "masked autoencoder has demonstrated its effectiveness in self-supervised point cloud learning. considering that masking is a kind of corruption, in this work we explore a more general denoising autoencoder for point cloud learning (point-dae) by investigating more types of corruptions beyond masking. specifically, we degrade the point cloud with certain corruptions as input, and learn an encoder-decoder model to reconstruct the original point cloud from its corrupted version. three corruption families (\\ie, density/masking, noise, and affine transformation) and a total of fourteen corruption types are investigated with traditional non-transformer encoders. besides the popular masking corruption, we identify another effective corruption family, \\ie, affine transformation. the affine transformation disturbs all points globally, which is complementary to the masking corruption where some local regions are dropped. we also validate the effectiveness of affine transformation corruption with the transformer backbones, where we decompose the reconstruction of the complete point cloud into the reconstructions of detailed local patches and rough global shape, alleviating the position leakage problem in the reconstruction. extensive experiments on tasks of object classification, few-shot learning, robustness testing, part segmentation, and 3d object detection validate the effectiveness of the proposed method. the codes are available at \\url{https://github.com/ybzh/point-dae}.",
        "doi": "",
        "created": "2022-11-13",
        "url": "https://arxiv.org/abs/2211.06841",
        "authors": [
            "yabin zhang",
            "jiehong lin",
            "ruihuang li",
            "kui jia",
            "lei zhang"
        ]
    },
    {
        "id": "2211.13316",
        "title": "understanding sample generation strategies for learning heuristic   functions in classical planning",
        "abstract": "we study the problem of learning good heuristic functions for classical planning tasks with neural networks based on samples represented by states with their cost-to-goal estimates. the heuristic function is learned for a state space and goal condition with the number of samples limited to a fraction of the size of the state space, and must generalize well for all states of the state space with the same goal condition. our main goal is to better understand the influence of sample generation strategies on the performance of a greedy best-first heuristic search (gbfs) guided by a learned heuristic function. in a set of controlled experiments, we find that two main factors determine the quality of the learned heuristic: which states are included in the sample set and the quality of the cost-to-goal estimates. these two factors are dependent: having perfect cost-to-goal estimates is insufficient if the samples are not well distributed across the state space. we also study other effects, such as adding samples with high-value estimates. based on our findings, we propose practical strategies to improve the quality of learned heuristics: three strategies that aim to generate more representative states and two strategies that improve the cost-to-goal estimates. our practical strategies almost double the mean coverage of a gbfs algorithm guided by a learned heuristic.",
        "doi": "",
        "created": "2022-11-23",
        "url": "https://arxiv.org/abs/2211.13316",
        "authors": [
            "r. v. bettker",
            "p. p. minini",
            "a. g. pereira",
            "m. ritt"
        ]
    },
    {
        "id": "2302.09270",
        "title": "towards safer generative language models: a survey on safety risks,   evaluations, and improvements",
        "abstract": "as generative large model capabilities advance, safety concerns become more pronounced in their outputs. to ensure the sustainable growth of the ai ecosystem, it's imperative to undertake a holistic evaluation and refinement of associated safety risks. this survey presents a framework for safety research pertaining to large models, delineating the landscape of safety risks as well as safety evaluation and improvement methods. we begin by introducing safety issues of wide concern, then delve into safety evaluation methods for large models, encompassing preference-based testing, adversarial attack approaches, issues detection, and other advanced evaluation methods. additionally, we explore the strategies for enhancing large model safety from training to deployment, highlighting cutting-edge safety approaches for each stage in building large models. finally, we discuss the core challenges in advancing towards more responsible ai, including the interpretability of safety mechanisms, ongoing safety issues, and robustness against malicious attacks. through this survey, we aim to provide clear technical guidance for safety researchers and encourage further study on the safety of large models.",
        "doi": "",
        "created": "2023-02-18",
        "url": "https://arxiv.org/abs/2302.09270",
        "authors": [
            "jiawen deng",
            "jiale cheng",
            "hao sun",
            "zhexin zhang",
            "minlie huang"
        ]
    },
    {
        "id": "2304.00962",
        "title": "regionplc: regional point-language contrastive learning for open-world   3d scene understanding",
        "abstract": "we propose a lightweight and scalable regional point-language contrastive learning framework, namely \\textbf{regionplc}, for open-world 3d scene understanding, aiming to identify and recognize open-set objects and categories. specifically, based on our empirical studies, we introduce a 3d-aware sfusion strategy that fuses 3d vision-language pairs derived from multiple 2d foundation models, yielding high-quality, dense region-level language descriptions without human 3d annotations. subsequently, we devise a region-aware point-discriminative contrastive learning objective to enable robust and effective 3d learning from dense regional language supervision. we carry out extensive experiments on scannet, scannet200, and nuscenes datasets, and our model outperforms prior 3d open-world scene understanding approaches by an average of 17.2\\% and 9.1\\% for semantic and instance segmentation, respectively, while maintaining greater scalability and lower resource demands. furthermore, our method has the flexibility to be effortlessly integrated with language models to enable open-ended grounded 3d reasoning without extra task-specific training. code will be released.",
        "doi": "",
        "created": "2023-04-03",
        "url": "https://arxiv.org/abs/2304.00962",
        "authors": [
            "jihan yang",
            "runyu ding",
            "weipeng deng",
            "zhe wang",
            "xiaojuan qi"
        ]
    },
    {
        "id": "2304.09991",
        "title": "supporting human-ai collaboration in auditing llms with llms",
        "abstract": "large language models are becoming increasingly pervasive and ubiquitous in society via deployment in sociotechnical systems. yet these language models, be it for classification or generation, have been shown to be biased and behave irresponsibly, causing harm to people at scale. it is crucial to audit these language models rigorously. existing auditing tools leverage either or both humans and ai to find failures. in this work, we draw upon literature in human-ai collaboration and sensemaking, and conduct interviews with research experts in safe and fair ai, to build upon the auditing tool: adatest (ribeiro and lundberg, 2022), which is powered by a generative large language model (llm). through the design process we highlight the importance of sensemaking and human-ai communication to leverage complementary strengths of humans and generative models in collaborative auditing. to evaluate the effectiveness of the augmented tool, adatest++, we conduct user studies with participants auditing two commercial language models: openai's gpt-3 and azure's sentiment analysis model. qualitative analysis shows that adatest++ effectively leverages human strengths such as schematization, hypothesis formation and testing. further, with our tool, participants identified a variety of failures modes, covering 26 different topics over 2 tasks, that have been shown before in formal audits and also those previously under-reported.",
        "doi": "10.1145/3600211.3604712",
        "created": "2023-04-19",
        "url": "https://arxiv.org/abs/2304.09991",
        "authors": [
            "charvi rastogi",
            "marco tulio ribeiro",
            "nicholas king",
            "harsha nori",
            "saleema amershi"
        ]
    },
    {
        "id": "2305.00162",
        "title": "beyond prediction: on-street parking recommendation using heterogeneous   graph-based list-wise ranking",
        "abstract": "to provide real-time parking information, existing studies focus on predicting parking availability, which seems an indirect approach to saving drivers' cruising time. in this paper, we first time propose an on-street parking recommendation (opr) task to directly recommend a parking space for a driver. to this end, a learn-to-rank (ltr) based opr model called opr-ltr is built. specifically, parking recommendation is closely related to the \"turnover events\" (state switching between occupied and vacant) of each parking space, and hence we design a highly efficient heterogeneous graph called esgraph to represent historical and real-time meters' turnover events as well as geographical relations; afterward, a convolution-based event-then-graph network is used to aggregate and update representations of the heterogeneous graph. a ranking model is further utilized to learn a score function that helps recommend a list of ranked parking spots for a specific on-street parking query. the method is verified using the on-street parking meter data in hong kong and san francisco. by comparing with the other two types of methods: prediction-only and prediction-then-recommendation, the proposed direct-recommendation method achieves satisfactory performance in different metrics. extensive experiments also demonstrate that the proposed esgraph and the recommendation model are more efficient in terms of computational efficiency as well as saving drivers' on-street parking time.",
        "doi": "",
        "created": "2023-04-28",
        "url": "https://arxiv.org/abs/2305.00162",
        "authors": [
            "hanyu sun",
            "xiao huang",
            "wei ma"
        ]
    },
    {
        "id": "2305.06988",
        "title": "self-chained image-language model for video localization and question   answering",
        "abstract": "recent studies have shown promising results on utilizing large pre-trained image-language models for video question answering. while these image-language models can efficiently bootstrap the representation learning of video-language models, they typically concatenate uniformly sampled video frames as visual inputs without explicit language-aware, temporal modeling. when only a portion of a video input is relevant to the language query, such uniform frame sampling can often lead to missing important visual cues. although humans often find a video moment to focus on and rewind the moment to answer questions, training a query-aware video moment localizer often requires expensive annotations and high computational costs. to address this issue, we propose self-chained video localization-answering (sevila), a novel framework that leverages a single image-language model (blip-2) to tackle both temporal keyframe localization and qa on videos. sevila framework consists of two modules: localizer and answerer, where both are parameter-efficiently fine-tuned from blip-2. we propose two ways of chaining these modules for cascaded inference and self-refinement. first, in the forward chain, the localizer finds multiple language-aware keyframes in a video, which the answerer uses to predict the answer. second, in the reverse chain, the answerer generates keyframe pseudo-labels to refine the localizer, alleviating the need for expensive video moment localization annotations. our sevila framework outperforms several strong baselines on 5 challenging video qa and event prediction benchmarks, and achieves the state-of-the-art in both fine-tuning (next-qa, star) and zero-shot (next-qa, star, how2qa, vlep) settings. we also analyze the impact of localizer, comparisons of localizer with other temporal localization models, pre-training/self-refinement of localizer, and varying the number of keyframes.",
        "doi": "",
        "created": "2023-05-11",
        "url": "https://arxiv.org/abs/2305.06988",
        "authors": [
            "shoubin yu",
            "jaemin cho",
            "prateek yadav",
            "mohit bansal"
        ]
    },
    {
        "id": "2305.13172",
        "title": "editing large language models: problems, methods, and opportunities",
        "abstract": "despite the ability to train capable llms, the methodology for maintaining their relevancy and rectifying errors remains elusive. to this end, the past few years have witnessed a surge in techniques for editing llms, the objective of which is to efficiently alter the behavior of llms within a specific domain without negatively impacting performance across other inputs. this paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for llms. in particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. we also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in making informed decisions on the selection of the most appropriate method for a specific task or context. code and datasets are available at https://github.com/zjunlp/easyedit.",
        "doi": "",
        "created": "2023-05-22",
        "url": "https://arxiv.org/abs/2305.13172",
        "authors": [
            "yunzhi yao",
            "peng wang",
            "bozhong tian",
            "siyuan cheng",
            "zhoubo li",
            "shumin deng",
            "huajun chen",
            "ningyu zhang"
        ]
    },
    {
        "id": "2305.13236",
        "title": "ada-gp: accelerating dnn training by adaptive gradient prediction",
        "abstract": "neural network training is inherently sequential where the layers finish the forward propagation in succession, followed by the calculation and back-propagation of gradients (based on a loss function) starting from the last layer. the sequential computations significantly slow down neural network training, especially the deeper ones. prediction has been successfully used in many areas of computer architecture to speed up sequential processing. therefore, we propose ada-gp, which uses gradient prediction adaptively to speed up deep neural network (dnn) training while maintaining accuracy. ada-gp works by incorporating a small neural network to predict gradients for different layers of a dnn model. ada-gp uses a novel tensor reorganization method to make it feasible to predict a large number of gradients. ada-gp alternates between dnn training using backpropagated gradients and dnn training using predicted gradients. ada-gp adaptively adjusts when and for how long gradient prediction is used to strike a balance between accuracy and performance. last but not least, we provide a detailed hardware extension in a typical dnn accelerator to realize the speed up potential from gradient prediction. our extensive experiments with fifteen dnn models show that ada-gp can achieve an average speed up of 1.47x with similar or even higher accuracy than the baseline models. moreover, it consumes, on average, 34% less energy due to reduced off-chip memory accesses compared to the baseline accelerator.",
        "doi": "",
        "created": "2023-05-22",
        "url": "https://arxiv.org/abs/2305.13236",
        "authors": [
            "vahid janfaza",
            "shantanu mandal",
            "farabi mahmud",
            "abdullah muzahid"
        ]
    },
    {
        "id": "2305.18498",
        "title": "anpl: towards natural programming with interactive decomposition",
        "abstract": "though llms are capable of generating plausible programs, it's challenging to interact with the llms further to revise the program, especially if the user's specific requirements are different from the initial proposal. in this paper, we introduce anpl, an interactive programming system that ensures users can always refine the generated code towards their specific programmatic intents via structured decompositions. borrowing the paradigm of sketching from program synthesis, an anpl program consists of a set of input-outputs that it must satisfy, a ``sketch'' -- control/data flow expressed in precise code (e.g. python), and ``holes'' -- sub-modules to be implemented by the llm specified with natural language. the user revises an anpl program by either modifying the sketch, changing the language used to describe the holes, or providing additional input-outputs to a particular hole, turning it into a sub-anpl program that can be solved recursively. this workflow allows the users to offload programming burdens to the llm as much as possible while retaining the ability to pinpoint and resolve bugs locally, without exposing the rest of the program to the llm. we deploy anpl on the abstraction and reasoning corpus (arc), a set of unique tasks that are challenging for state-of-the-art ai systems, showing it outperforms baseline programming systems that (a) without the ability to decompose tasks interactively and (b) without the guarantee that the modules can be correctly composed together. additional evaluations on apps, humaneval, and real-world programming tasks have validated that the anpl framework is applicable to multiple programming domains. we release the anpl solutions to the arc tasks as a dataset, providing insights into how humans decompose novel tasks programmatically. see our code at https://iprc-dip.github.io/anpl/.",
        "doi": "",
        "created": "2023-05-29",
        "url": "https://arxiv.org/abs/2305.18498",
        "authors": [
            "di huang",
            "ziyuan nan",
            "xing hu",
            "pengwei jin",
            "shaohui peng",
            "yuanbo wen",
            "rui zhang",
            "zidong du",
            "qi guo",
            "yewen pu",
            "yunji chen"
        ]
    },
    {
        "id": "2306.01286",
        "title": "kl-divergence guided temperature sampling",
        "abstract": "temperature sampling is a conventional approach to diversify large language model predictions. as temperature increases, the prediction becomes diverse but also vulnerable to hallucinations -- generating tokens that are sensible but not factual. one common approach to mitigate hallucinations is to provide source/grounding documents and the model is trained to produce predictions that bind to and are attributable to the provided source. it appears that there is a trade-off between diversity and attribution. to mitigate any such trade-off, we propose to relax the constraint of having a fixed temperature over decoding steps, and a mechanism to guide the dynamic temperature according to its relevance to the source through kl-divergence. our experiments justifies the trade-off, and shows that our sampling algorithm outperforms the conventional top-k and top-p algorithms in conversational question-answering and summarization tasks.",
        "doi": "",
        "created": "2023-06-02",
        "url": "https://arxiv.org/abs/2306.01286",
        "authors": [
            "chung-ching chang",
            "david reitter",
            "renat aksitov",
            "yun-hsuan sung"
        ]
    },
    {
        "id": "2306.03263",
        "title": "efficient automatic design of robots",
        "abstract": "robots are notoriously difficult to design because of complex interdependencies between their physical structure, sensory and motor layouts, and behavior. despite this, almost every detail of every robot built to date has been manually determined by a human designer after several months or years of iterative ideation, prototyping, and testing. inspired by evolutionary design in nature, the automated design of robots using evolutionary algorithms has been attempted for two decades, but it too remains inefficient: days of supercomputing are required to design robots in simulation that, when manufactured, exhibit desired behavior. here we show for the first time de-novo optimization of a robot's structure to exhibit a desired behavior, within seconds on a single consumer-grade computer, and the manufactured robot's retention of that behavior. unlike other gradient-based robot design methods, this algorithm does not presuppose any particular anatomical form; starting instead from a randomly-generated apodous body plan, it consistently discovers legged locomotion, the most efficient known form of terrestrial movement. if combined with automated fabrication and scaled up to more challenging tasks, this advance promises near instantaneous design, manufacture, and deployment of unique and useful machines for medical, environmental, vehicular, and space-based tasks.",
        "doi": "10.1073/pnas.2305180120",
        "created": "2023-06-05",
        "url": "https://arxiv.org/abs/2306.03263",
        "authors": [
            "david matthews",
            "andrew spielberg",
            "daniela rus",
            "sam kriegman",
            "josh bongard"
        ]
    },
    {
        "id": "2306.04541",
        "title": "top-down knowledge compilation for counting modulo theories",
        "abstract": "propositional model counting (#sat) can be solved efficiently when the input formula is in deterministic decomposable negation normal form (d-dnnf). translating an arbitrary formula into a representation that allows inference tasks, such as counting, to be performed efficiently, is called knowledge compilation. top-down knowledge compilation is a state-of-the-art technique for solving #sat problems that leverages the traces of exhaustive dpll search to obtain d-dnnf representations. while knowledge compilation is well studied for propositional approaches, knowledge compilation for the (quantifier free) counting modulo theory setting (#smt) has been studied to a much lesser degree. in this paper, we discuss compilation strategies for #smt. we specifically advocate for a top-down compiler based on the traces of exhaustive dpll(t) search.",
        "doi": "",
        "created": "2023-06-07",
        "url": "https://arxiv.org/abs/2306.04541",
        "authors": [
            "vincent derkinderen",
            "pedro zuidberg dos martires",
            "samuel kolb",
            "paolo morettin"
        ]
    },
    {
        "id": "2306.05846",
        "title": "motion-dvae: unsupervised learning for fast human motion denoising",
        "abstract": "pose and motion priors are crucial for recovering realistic and accurate human motion from noisy observations. substantial progress has been made on pose and shape estimation from images, and recent works showed impressive results using priors to refine frame-wise predictions. however, a lot of motion priors only model transitions between consecutive poses and are used in time-consuming optimization procedures, which is problematic for many applications requiring real-time motion capture. we introduce motion-dvae, a motion prior to capture the short-term dependencies of human motion. as part of the dynamical variational autoencoder (dvae) models family, motion-dvae combines the generative capability of vae models and the temporal modeling of recurrent architectures. together with motion-dvae, we introduce an unsupervised learned denoising method unifying regression- and optimization-based approaches in a single framework for real-time 3d human pose estimation. experiments show that the proposed approach reaches competitive performance with state-of-the-art methods while being much faster.",
        "doi": "",
        "created": "2023-06-09",
        "url": "https://arxiv.org/abs/2306.05846",
        "authors": [
            "gu\u00e9nol\u00e9 fiche",
            "simon leglaive",
            "xavier alameda-pineda",
            "renaud s\u00e9guier"
        ]
    },
    {
        "id": "2306.07266",
        "title": "operator learning with neural fields: tackling pdes on general   geometries",
        "abstract": "machine learning approaches for solving partial differential equations require learning mappings between function spaces. while convolutional or graph neural networks are constrained to discretized functions, neural operators present a promising milestone toward mapping functions directly. despite impressive results they still face challenges with respect to the domain geometry and typically rely on some form of discretization. in order to alleviate such limitations, we present coral, a new method that leverages coordinate-based networks for solving pdes on general geometries. coral is designed to remove constraints on the input mesh, making it applicable to any spatial sampling and geometry. its ability extends to diverse problem domains, including pde solving, spatio-temporal forecasting, and inverse problems like geometric design. coral demonstrates robust performance across multiple resolutions and performs well in both convex and non-convex domains, surpassing or performing on par with state-of-the-art models.",
        "doi": "",
        "created": "2023-06-12",
        "url": "https://arxiv.org/abs/2306.07266",
        "authors": [
            "louis serrano",
            "lise le boudec",
            "armand kassa\u00ef koupa\u00ef",
            "thomas x wang",
            "yuan yin",
            "jean-no\u00ebl vittaut",
            "patrick gallinari"
        ]
    },
    {
        "id": "2306.08018",
        "title": "mol-instructions: a large-scale biomolecular instruction dataset for   large language models",
        "abstract": "large language models (llms), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. however, their proficiency within specialized domains such as biomolecular studies remains limited. to address this challenge, we introduce mol-instructions, a comprehensive instruction dataset designed for the biomolecular domain. mol-instructions encompasses three key components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions. each component aims to improve the understanding and prediction capabilities of llms concerning biomolecular features and behaviors. through extensive instruction tuning experiments on llms, we demonstrate the effectiveness of mol-instructions in enhancing large models' performance in the intricate realm of biomolecular studies, thus fostering progress in the biomolecular research community. mol-instructions is publicly available for ongoing research and will undergo regular updates to enhance its applicability.",
        "doi": "",
        "created": "2023-06-13",
        "url": "https://arxiv.org/abs/2306.08018",
        "authors": [
            "yin fang",
            "xiaozhuan liang",
            "ningyu zhang",
            "kangwei liu",
            "rui huang",
            "zhuo chen",
            "xiaohui fan",
            "huajun chen"
        ]
    },
    {
        "id": "2306.10012",
        "title": "magicbrush: a manually annotated dataset for instruction-guided image   editing",
        "abstract": "text-guided image editing is widely needed in daily life, ranging from personal use to professional applications such as photoshop. however, existing methods are either zero-shot or trained on an automatically synthesized dataset, which contains a high volume of noise. thus, they still require lots of manual tuning to produce desirable outcomes in practice. to address this issue, we introduce magicbrush (https://osu-nlp-group.github.io/magicbrush/), the first large-scale, manually annotated dataset for instruction-guided real image editing that covers diverse scenarios: single-turn, multi-turn, mask-provided, and mask-free editing. magicbrush comprises over 10k manually annotated triplets (source image, instruction, target image), which supports trainining large-scale text-guided image editing models. we fine-tune instructpix2pix on magicbrush and show that the new model can produce much better images according to human evaluation. we further conduct extensive experiments to evaluate current image editing baselines from multiple dimensions including quantitative, qualitative, and human evaluations. the results reveal the challenging nature of our dataset and the gap between current baselines and real-world editing needs.",
        "doi": "",
        "created": "2023-06-16",
        "url": "https://arxiv.org/abs/2306.10012",
        "authors": [
            "kai zhang",
            "lingbo mo",
            "wenhu chen",
            "huan sun",
            "yu su"
        ]
    },
    {
        "id": "2306.12230",
        "title": "fantastic weights and how to find them: where to prune in dynamic sparse   training",
        "abstract": "dynamic sparse training (dst) is a rapidly evolving area of research that seeks to optimize the sparse initialization of a neural network by adapting its topology during training. it has been shown that under specific conditions, dst is able to outperform dense models. the key components of this framework are the pruning and growing criteria, which are repeatedly applied during the training process to adjust the network's sparse connectivity. while the growing criterion's impact on dst performance is relatively well studied, the influence of the pruning criterion remains overlooked. to address this issue, we design and perform an extensive empirical analysis of various pruning criteria to better understand their impact on the dynamics of dst solutions. surprisingly, we find that most of the studied methods yield similar results. the differences become more significant in the low-density regime, where the best performance is predominantly given by the simplest technique: magnitude-based pruning. the code is provided at https://github.com/alooow/fantastic_weights_paper",
        "doi": "",
        "created": "2023-06-21",
        "url": "https://arxiv.org/abs/2306.12230",
        "authors": [
            "aleksandra i. nowak",
            "bram grooten",
            "decebal constantin mocanu",
            "jacek tabor"
        ]
    },
    {
        "id": "2307.03170",
        "title": "focused transformer: contrastive training for context scaling",
        "abstract": "large language models have an exceptional capability to incorporate new information in a contextual manner. however, the full potential of such an approach is often restrained due to a limitation in the effective context length. one solution to this issue is to endow an attention layer with access to an external memory, which comprises of (key, value) pairs. yet, as the number of documents increases, the proportion of relevant keys to irrelevant ones decreases, leading the model to focus more on the irrelevant keys. we identify a significant challenge, dubbed the distraction issue, where keys linked to different semantic values might overlap, making them hard to distinguish. to tackle this problem, we introduce the focused transformer (fot), a technique that employs a training process inspired by contrastive learning. this novel approach enhances the structure of the (key, value) space, enabling an extension of the context length. our method allows for fine-tuning pre-existing, large-scale models to lengthen their effective context. this is demonstrated by our fine-tuning of $3b$ and $7b$ openllama checkpoints. the resulting models, which we name longllama, exhibit advancements in tasks requiring a long context. we further illustrate that our longllama models adeptly manage a $256 k$ context length for passkey retrieval.",
        "doi": "",
        "created": "2023-07-06",
        "url": "https://arxiv.org/abs/2307.03170",
        "authors": [
            "szymon tworkowski",
            "konrad staniszewski",
            "miko\u0142aj pacek",
            "yuhuai wu",
            "henryk michalewski",
            "piotr mi\u0142o\u015b"
        ]
    },
    {
        "id": "2307.03913",
        "title": "applying hcai in developing effective human-ai teaming: a perspective   from human-ai joint cognitive systems",
        "abstract": "research and application have used human-ai teaming (hat) as a new paradigm to develop ai systems. hat recognizes that ai will function as a teammate instead of simply a tool in collaboration with humans. effective human-ai teams need to be capable of taking advantage of the unique abilities of both humans and ai while overcoming the known challenges and limitations of each member, augmenting human capabilities, and raising joint performance beyond that of either entity. the national ai research and strategic plan 2023 update has recognized that research programs focusing primarily on the independent performance of ai systems generally fail to consider the functionality that ai must provide within the context of dynamic, adaptive, and collaborative teams and calls for further research on human-ai teaming and collaboration. however, there has been debate about whether ai can work as a teammate with humans. the primary concern is that adopting the \"teaming\" paradigm contradicts the human-centered ai (hcai) approach, resulting in humans losing control of ai systems. this article further analyzes the hat paradigm and the debates. specifically, we elaborate on our proposed conceptual framework of human-ai joint cognitive systems (haijcs) and apply it to represent hat under the hcai umbrella. we believe that haijcs may help adopt hai while enabling hcai. the implications and future work for haijcs are also discussed.   insights: ai has led to the emergence of a new form of human-machine relationship: human-ai teaming (hat), a paradigmatic shift in human-ai systems; we must follow a human-centered ai (hcai) approach when applying hat as a new design paradigm; we propose a conceptual framework of human-ai joint cognitive systems (haijcs) to represent and implement hat for developing effective human-ai teaming",
        "doi": "",
        "created": "2023-07-08",
        "url": "https://arxiv.org/abs/2307.03913",
        "authors": [
            "wei xu",
            "zaifeng gao"
        ]
    },
    {
        "id": "2308.00629",
        "title": "hessian-aware bayesian optimization for decision making systems",
        "abstract": "many approaches for optimizing decision making systems rely on gradient based methods requiring informative feedback from the environment. however, in the case where such feedback is sparse or uninformative, such approaches may result in poor performance. derivative-free approaches such as bayesian optimization mitigate the dependency on the quality of gradient feedback, but are known to scale poorly in the high-dimension setting of complex decision making systems. this problem is exacerbated if the system requires interactions between several actors cooperating to accomplish a shared goal. to address the dimensionality challenge, we propose a compact multi-layered architecture modeling the dynamics of actor interactions through the concept of role. additionally, we introduce hessian-aware bayesian optimization to efficiently optimize the multi-layered architecture parameterized by a large number of parameters. experimental results demonstrate that our method (ha-gp-ucb) works effectively on several benchmarks under resource constraints and malformed feedback settings.",
        "doi": "",
        "created": "2023-08-01",
        "url": "https://arxiv.org/abs/2308.00629",
        "authors": [
            "mohit rajpal",
            "lac gia tran",
            "yehong zhang",
            "bryan kian hsiang low"
        ]
    },
    {
        "id": "2308.10997",
        "title": "markovgen: structured prediction for efficient text-to-image generation",
        "abstract": "modern text-to-image generation models produce high-quality images that are both photorealistic and faithful to the text prompts. however, this quality comes at significant computational cost: nearly all of these models are iterative and require running sampling multiple times with large models. this iterative process is needed to ensure that different regions of the image are not only aligned with the text prompt, but also compatible with each other. in this work, we propose a light-weight approach to achieving this compatibility between different regions of an image, using a markov random field (mrf) model. we demonstrate the effectiveness of this method on top of the latent token-based muse text-to-image model. the mrf richly encodes the compatibility among image tokens at different spatial locations to improve quality and significantly reduce the required number of muse sampling steps. inference with the mrf is significantly cheaper, and its parameters can be quickly learned through back-propagation by modeling mrf inference as a differentiable neural-network layer. our full model, markovgen, uses this proposed mrf model to both speed up muse by 1.5x and produce higher quality images by decreasing undesirable image artifacts.",
        "doi": "",
        "created": "2023-08-14",
        "url": "https://arxiv.org/abs/2308.10997",
        "authors": [
            "sadeep jayasumana",
            "daniel glasner",
            "srikumar ramalingam",
            "andreas veit",
            "ayan chakrabarti",
            "sanjiv kumar"
        ]
    },
    {
        "id": "2309.03720",
        "title": "a natural gas consumption forecasting system for continual learning   scenarios based on hoeffding trees with change point detection mechanism",
        "abstract": "forecasting natural gas consumption, considering seasonality and trends, is crucial in planning its supply and consumption and optimizing the cost of obtaining it, mainly by industrial entities. however, in times of threats to its supply, it is also a critical element that guarantees the supply of this raw material to meet individual consumers' needs, ensuring society's energy security. this article introduces a novel multistep ahead forecasting of natural gas consumption with change point detection integration for model collection selection with continual learning capabilities using data stream processing. the performance of the forecasting models based on the proposed approach is evaluated in a complex real-world use case of natural gas consumption forecasting. we employed hoeffding tree predictors as forecasting models and the pruned exact linear time (pelt) algorithm for the change point detection procedure. the change point detection integration enables selecting a different model collection for successive time frames. thus, three model collection selection procedures (with and without an error feedback loop) are defined and evaluated for forecasting scenarios with various densities of detected change points. these models were compared with change point agnostic baseline approaches. our experiments show that fewer change points result in a lower forecasting error regardless of the model collection selection procedure employed. also, simpler model collection selection procedures omitting forecasting error feedback leads to more robust forecasting models suitable for continual learning tasks.",
        "doi": "",
        "created": "2023-09-07",
        "url": "https://arxiv.org/abs/2309.03720",
        "authors": [
            "radek svoboda",
            "sebastian basterrech",
            "j\u0119drzej kozal",
            "jan plato\u0161",
            "micha\u0142 wo\u017aniak"
        ]
    },
    {
        "id": "2309.09737",
        "title": "moving object detection and tracking with 4d radar point cloud",
        "abstract": "mobile autonomy relies on the precise perception of dynamic environments. robustly tracking moving objects in 3d world thus plays a pivotal role for applications like trajectory prediction, obstacle avoidance, and path planning. while most current methods utilize lidars or cameras for multiple object tracking (mot), the capabilities of 4d imaging radars remain largely unexplored. recognizing the challenges posed by radar noise and point sparsity in 4d radar data, we introduce ratrack, an innovative solution tailored for radar-based tracking. bypassing the typical reliance on specific object types and 3d bounding boxes, our method focuses on motion segmentation and clustering, enriched by a motion estimation module. evaluated on the view-of-delft dataset, ratrack showcases superior tracking precision of moving objects, largely surpassing the performance of the state of the art.",
        "doi": "",
        "created": "2023-09-18",
        "url": "https://arxiv.org/abs/2309.09737",
        "authors": [
            "zhijun pan",
            "fangqiang ding",
            "hantao zhong",
            "chris xiaoxuan lu"
        ]
    },
    {
        "id": "2309.09809",
        "title": "a continual learning paradigm for non-differentiable visual programming   frameworks on visual reasoning tasks",
        "abstract": "recently, the visual programming framework (visprog) has emerged as a significant framework for executing compositional visual tasks due to its interpretability and flexibility. however, the performance of visprog on specific visual reasoning (vr) tasks is markedly inferior compared to well-trained task-specific models since its employed visual sub-modules have limited generalization capabilities. due to the non-differentiability of visprog, it is quite challenging to improve these visual sub-modules within visprog for the specific vr task while maintaining their generalizability on the un-seen tasks. attempt to overcome these difficulties, we propose clvp, a continuous learning paradigm for visprog across various visual reasoning tasks. specifically, our clvp distills the capabilities of well-trained task-specific models into the visual sub-modules in a stepwise and anti-forgetting manner. this can continually improve the performance of visprog on multiple visual tasks while preserving the flexibility of visprog. extensive and comprehensive experimental results demonstrate that our clvp obtains significant performance gains on specific vr benchmarks, i.e., gqa (+1.4%) and nlvrv2 (+5.6%), compared to the visprog baseline, and also maintains a promising generalizability for vr on un-seen and previous learned tasks.",
        "doi": "",
        "created": "2023-09-18",
        "url": "https://arxiv.org/abs/2309.09809",
        "authors": [
            "wentao wan",
            "nan kang",
            "zeqing wang",
            "zhuojie yang",
            "liang lin",
            "keze wang"
        ]
    },
    {
        "id": "2309.11526",
        "title": "likelihood-based sensor calibration using affine transformation",
        "abstract": "an important task in the field of sensor technology is the efficient implementation of adaptation procedures of measurements from one sensor to another sensor of identical design. one idea is to use the estimation of an affine transformation between different systems, which can be improved by the knowledge of experts. this paper presents an improved solution from glacier research that was published back in 1973. the results demonstrate the adaptability of this solution for various applications, including software calibration of sensors, implementation of expert-based adaptation, and paving the way for future advancements such as distributed learning methods. one idea here is to use the knowledge of experts for estimating an affine transformation between different systems. we evaluate our research with simulations and also with real measured data of a multi-sensor board with 8 identical sensors. both data set and evaluation script are provided for download. the results show an improvement for both the simulation and the experiments with real data.",
        "doi": "",
        "created": "2023-09-20",
        "url": "https://arxiv.org/abs/2309.11526",
        "authors": [
            "r\u00fcdiger machhamer",
            "lejla begic fazlic",
            "eray guven",
            "david junk",
            "gunes karabulut kurt",
            "stefan naumann",
            "stephan didas",
            "klaus-uwe gollmer",
            "ralph bergmann",
            "ingo j. timm",
            "guido dartmann"
        ]
    },
    {
        "id": "2309.12673",
        "title": "on sparse modern hopfield model",
        "abstract": "we introduce the sparse modern hopfield model as a sparse extension of the modern hopfield model. like its dense counterpart, the sparse modern hopfield model equips a memory-retrieval dynamics whose one-step approximation corresponds to the sparse attention mechanism. theoretically, our key contribution is a principled derivation of a closed-form sparse hopfield energy using the convex conjugate of the sparse entropic regularizer. building upon this, we derive the sparse memory retrieval dynamics from the sparse energy function and show its one-step approximation is equivalent to the sparse-structured attention. importantly, we provide a sparsity-dependent memory retrieval error bound which is provably tighter than its dense analog. the conditions for the benefits of sparsity to arise are therefore identified and discussed. in addition, we show that the sparse modern hopfield model maintains the robust theoretical properties of its dense counterpart, including rapid fixed point convergence and exponential memory capacity. empirically, we use both synthetic and real-world datasets to demonstrate that the sparse hopfield model outperforms its dense counterpart in many situations.",
        "doi": "",
        "created": "2023-09-22",
        "url": "https://arxiv.org/abs/2309.12673",
        "authors": [
            "jerry yao-chieh hu",
            "donglin yang",
            "dennis wu",
            "chenwei xu",
            "bo-yu chen",
            "han liu"
        ]
    },
    {
        "id": "2309.12677",
        "title": "trtr: a versatile pre-trained large traffic model based on transformer   for capturing trajectory diversity in vehicle population",
        "abstract": "understanding trajectory diversity is a fundamental aspect of addressing practical traffic tasks. however, capturing the diversity of trajectories presents challenges, particularly with traditional machine learning and recurrent neural networks due to the requirement of large-scale parameters. the emerging transformer technology, renowned for its parallel computation capabilities enabling the utilization of models with hundreds of millions of parameters, offers a promising solution. in this study, we apply the transformer architecture to traffic tasks, aiming to learn the diversity of trajectories within vehicle populations. we analyze the transformer's attention mechanism and its adaptability to the goals of traffic tasks, and subsequently, design specific pre-training tasks. to achieve this, we create a data structure tailored to the attention mechanism and introduce a set of noises that correspond to spatio-temporal demands, which are incorporated into the structured data during the pre-training process. the designed pre-training model demonstrates excellent performance in capturing the spatial distribution of the vehicle population, with no instances of vehicle overlap and an rmse of 0.6059 when compared to the ground truth values. in the context of time series prediction, approximately 95% of the predicted trajectories' speeds closely align with the true speeds, within a deviation of 7.5144m/s. furthermore, in the stability test, the model exhibits robustness by continuously predicting a time series ten times longer than the input sequence, delivering smooth trajectories and showcasing diverse driving behaviors. the pre-trained model also provides a good basis for downstream fine-tuning tasks. the number of parameters of our model is over 50 million.",
        "doi": "",
        "created": "2023-09-22",
        "url": "https://arxiv.org/abs/2309.12677",
        "authors": [
            "ruyi feng",
            "zhibin li",
            "bowen liu",
            "yan ding"
        ]
    },
    {
        "id": "2309.16573",
        "title": "language models as a service: overview of a new paradigm and its   challenges",
        "abstract": "some of the most powerful language models currently are proprietary systems, accessible only via (typically restrictive) web or software programming interfaces. this is the language-models-as-a-service (lmaas) paradigm. in contrast with scenarios where full model access is available, as in the case of open-source models, such closed-off language models present specific challenges for evaluating, benchmarking, and testing them. this paper has two goals: on the one hand, we delineate how the aforementioned challenges act as impediments to the accessibility, replicability, reliability, and trustworthiness of lmaas. we systematically examine the issues that arise from a lack of information about language models for each of these four aspects. we conduct a detailed analysis of existing solutions and put forth a number of considered recommendations, and highlight the directions for future advancements. on the other hand, it serves as a comprehensive resource for existing knowledge on current, major lmaas, offering a synthesized overview of the licences and capabilities their interfaces offer.",
        "doi": "",
        "created": "2023-09-28",
        "url": "https://arxiv.org/abs/2309.16573",
        "authors": [
            "emanuele la malfa",
            "aleksandar petrov",
            "simon frieder",
            "christoph weinhuber",
            "ryan burnell",
            "raza nazar",
            "anthony g. cohn",
            "nigel shadbolt",
            "michael wooldridge"
        ]
    },
    {
        "id": "2310.00533",
        "title": "self: language-driven self-evolution for large language models",
        "abstract": "large language models (llms) have demonstrated remarkable versatility across various domains. to further advance llms, we propose 'self' (self-evolution with language feedback), a novel approach that enables llms to self-improve through self-reflection, akin to human learning processes. self initiates with a meta-skill learning process that equips the llms with capabilities for self-feedback and self-refinement. subsequently, the model undergoes an iterative process of self-evolution. in each iteration, it utilizes an unlabeled dataset of instructions to generate initial responses. these responses are enhanced through self-feedback and self-refinement. the model is then fine-tuned using this enhanced data. the model undergoes progressive improvement through this iterative self-evolution process. moreover, the self framework enables the model to apply self-refinement during inference, which further improves response quality. our experiments in mathematics and general tasks demonstrate that self can enhance the capabilities of llms without human intervention. the self framework indicates a promising direction for the autonomous evolution of llms, transitioning them from passive information receivers to active participants in their development.",
        "doi": "",
        "created": "2023-09-30",
        "url": "https://arxiv.org/abs/2310.00533",
        "authors": [
            "jianqiao lu",
            "wanjun zhong",
            "wenyong huang",
            "yufei wang",
            "fei mi",
            "baojun wang",
            "weichao wang",
            "lifeng shang",
            "qun liu"
        ]
    },
    {
        "id": "2310.05518",
        "title": "on double descent in reinforcement learning with lstd and random   features",
        "abstract": "temporal difference (td) algorithms are widely used in deep reinforcement learning (rl). their performance is heavily influenced by the size of the neural network. while in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in rl is much less clear. in this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. we identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. furthermore, we observe a double descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. leveraging random features and the lazy training regime, we study the regularized least-square temporal difference (lstd) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. we derive deterministic limits of both the empirical and the true mean-square bellman error (msbe) that feature correction terms responsible for the double-descent. correction terms vanish when the $l_2$-regularization is increased or the number of unvisited states goes to zero. numerical experiments with synthetic and small real-world environments closely match the theoretical predictions.",
        "doi": "",
        "created": "2023-10-09",
        "url": "https://arxiv.org/abs/2310.05518",
        "authors": [
            "david brellmann",
            "elo\u00efse berthier",
            "david filliat",
            "goran frehse"
        ]
    },
    {
        "id": "2310.05566",
        "title": "aggregated f-average neural network for interpretable ensembling",
        "abstract": "ensemble learning leverages multiple models (i.e., weak learners) on a common machine learning task to enhance prediction performance. basic ensembling approaches average the weak learners outputs, while more sophisticated ones stack a machine learning model in between the weak learners outputs and the final prediction. this work fuses both aforementioned frameworks. we introduce an aggregated f-average (afa) shallow neural network which models and combines different types of averages to perform an optimal aggregation of the weak learners predictions. we emphasise its interpretable architecture and simple training strategy, and illustrate its good performance on the problem of few-shot class incremental learning.",
        "doi": "",
        "created": "2023-10-09",
        "url": "https://arxiv.org/abs/2310.05566",
        "authors": [
            "mathieu vu",
            "emilie chouzenoux",
            "jean-christophe pesquet",
            "ismail ben ayed"
        ]
    },
    {
        "id": "2310.07918",
        "title": "contextualized policy recovery: modeling and interpreting medical   decisions with adaptive imitation learning",
        "abstract": "interpretable policy learning seeks to estimate intelligible decision policies from observed actions; however, existing models fall short by forcing a tradeoff between accuracy and interpretability. this tradeoff limits data-driven interpretations of human decision-making process. e.g. to audit medical decisions for biases and suboptimal practices, we require models of decision processes which provide concise descriptions of complex behaviors. fundamentally, existing approaches are burdened by this tradeoff because they represent the underlying decision process as a universal policy, when in fact human decisions are dynamic and can change drastically with contextual information. thus, we propose contextualized policy recovery (cpr), which re-frames the problem of modeling complex decision processes as a multi-task learning problem in which complex decision policies are comprised of context-specific policies. cpr models each context-specific policy as a linear observation-to-action mapping, and generates new decision models $\\textit{on-demand}$ as contexts are updated with new observations. cpr is compatible with fully offline and partially observable decision environments, and can be tailored to incorporate any recurrent black-box model or interpretable decision model. we assess cpr through studies on simulated and real data, achieving state-of-the-art performance on the canonical tasks of predicting antibiotic prescription in intensive care units ($+22\\%$ auroc vs. previous sota) and predicting mri prescription for alzheimer's patients ($+7.7\\%$ auroc vs. previous sota). with this improvement in predictive performance, cpr closes the accuracy gap between interpretable and black-box methods for policy learning, allowing high-resolution exploration and analysis of context-specific decision models.",
        "doi": "",
        "created": "2023-10-11",
        "url": "https://arxiv.org/abs/2310.07918",
        "authors": [
            "jannik deuschel",
            "caleb n. ellington",
            "benjamin j. lengerich",
            "yingtao luo",
            "pascal friederich",
            "eric p. xing"
        ]
    },
    {
        "id": "2310.08540",
        "title": "do pretrained transformers really learn in-context by gradient descent?",
        "abstract": "the emergence of in-context learning (icl) in llms remains a significant phenomenon with little understanding. to explain icl, recent studies try to shed light on icl by connecting it to gradient descent (gd). however, the question is, do these hold up in practice in actual pre-trained models?   we highlight the limiting assumptions in prior works that make their context considerably different from the practical context in which language models are trained. for example, the theoretical hand-constructed weights used in these studies have properties that don't match those of real llms. furthermore, their experimental verification uses \\emph{icl objective} (training models explicitly for icl), which differs from the emergent icl in the wild.   we also look for evidence in real models. we observe that icl and gd have different sensitivity to the order in which they observe demonstrations. finally, we probe and compare the icl vs. gd hypothesis in a natural setting. we conduct comprehensive empirical analyses on language models pre-trained on natural data (llama-7b). our comparisons of three performance metrics highlight the inconsistent behavior of icl and gd as a function of various factors such as datasets, models, and the number of demonstrations. we observe that icl and gd modify the output distribution of language models differently. these results indicate that the equivalence between icl and gd remains an open hypothesis and calls for further studies.",
        "doi": "",
        "created": "2023-10-12",
        "url": "https://arxiv.org/abs/2310.08540",
        "authors": [
            "lingfeng shen",
            "aayush mishra",
            "daniel khashabi"
        ]
    },
    {
        "id": "2310.15065",
        "title": "synergizing human-ai agency: a guide of 23 heuristics for service   co-creation with llm-based agents",
        "abstract": "this empirical study serves as a primer for interested service providers to determine if and how large language models (llms) technology will be integrated for their practitioners and the broader community. we investigate the mutual learning journey of non-ai experts and ai through coagent, a service co-creation tool with llm-based agents. engaging in a three-stage participatory design processes, we work with with 23 domain experts from public libraries across the u.s., uncovering their fundamental challenges of integrating ai into human workflows. our findings provide 23 actionable \"heuristics for service co-creation with ai\", highlighting the nuanced shared responsibilities between humans and ai. we further exemplar 9 foundational agency aspects for ai, emphasizing essentials like ownership, fair treatment, and freedom of expression. our innovative approach enriches the participatory design model by incorporating ai as crucial stakeholders and utilizing ai-ai interaction to identify blind spots. collectively, these insights pave the way for synergistic and ethical human-ai co-creation in service contexts, preparing for workforce ecosystems where ai coexists.",
        "doi": "",
        "created": "2023-10-23",
        "url": "https://arxiv.org/abs/2310.15065",
        "authors": [
            "qingxiao zheng",
            "zhongwei xu",
            "abhinav choudhry",
            "yuting chen",
            "yongming li",
            "yun huang"
        ]
    },
    {
        "id": "2310.17017",
        "title": "an integrative survey on mental health conversational agents to bridge   computer science and medical perspectives",
        "abstract": "mental health conversational agents (a.k.a. chatbots) are widely studied for their potential to offer accessible support to those experiencing mental health challenges. previous surveys on the topic primarily consider papers published in either computer science or medicine, leading to a divide in understanding and hindering the sharing of beneficial knowledge between both domains. to bridge this gap, we conduct a comprehensive literature review using the prisma framework, reviewing 534 papers published in both computer science and medicine. our systematic review reveals 136 key papers on building mental health-related conversational agents with diverse characteristics of modeling and experimental design techniques. we find that computer science papers focus on llm techniques and evaluating response quality using automated metrics with little attention to the application while medical papers use rule-based conversational agents and outcome metrics to measure the health outcomes of participants. based on our findings on transparency, ethics, and cultural heterogeneity in this review, we provide a few recommendations to help bridge the disciplinary divide and enable the cross-disciplinary development of mental health conversational agents.",
        "doi": "",
        "created": "2023-10-25",
        "url": "https://arxiv.org/abs/2310.17017",
        "authors": [
            "young min cho",
            "sunny rai",
            "lyle ungar",
            "jo\u00e3o sedoc",
            "sharath chandra guntuku"
        ]
    },
    {
        "id": "2310.17940",
        "title": "unified segment-to-segment framework for simultaneous sequence   generation",
        "abstract": "simultaneous sequence generation is a pivotal task for real-time scenarios, such as streaming speech recognition, simultaneous machine translation and simultaneous speech translation, where the target sequence is generated while receiving the source sequence. the crux of achieving high-quality generation with low latency lies in identifying the optimal moments for generating, accomplished by learning a mapping between the source and target sequences. however, existing methods often rely on task-specific heuristics for different sequence types, limiting the model's capacity to adaptively learn the source-target mapping and hindering the exploration of multi-task learning for various simultaneous tasks. in this paper, we propose a unified segment-to-segment framework (seg2seg) for simultaneous sequence generation, which learns the mapping in an adaptive and unified manner. during the process of simultaneous generation, the model alternates between waiting for a source segment and generating a target segment, making the segment serve as the natural bridge between the source and target. to accomplish this, seg2seg introduces a latent segment as the pivot between source to target and explores all potential source-target mappings via the proposed expectation training, thereby learning the optimal moments for generating. experiments on multiple simultaneous generation tasks demonstrate that seg2seg achieves state-of-the-art performance and exhibits better generality across various tasks.",
        "doi": "",
        "created": "2023-10-27",
        "url": "https://arxiv.org/abs/2310.17940",
        "authors": [
            "shaolei zhang",
            "yang feng"
        ]
    },
    {
        "id": "2311.01723",
        "title": "towards calibrated robust fine-tuning of vision-language models",
        "abstract": "while fine-tuning unlocks the potential of a pre-trained model for a specific task, it compromises the model's ability to generalize to out-of-distribution (ood) datasets. to mitigate this, robust fine-tuning aims to ensure performance on ood datasets as well as on an in-distribution (id) dataset for which the model is being tuned. however, another criterion for reliable machine learning (ml), confidence calibration, has been overlooked despite its increasing demand for real-world high-stakes ml applications (e.g., autonomous driving and medical diagnosis). for the first time, we raise concerns about the calibration of fine-tuned vision-language models (vlms) under distribution shift by showing that naive fine-tuning and even state-of-the-art robust fine-tuning methods hurt the calibration of pre-trained vlms, especially on ood datasets. to address this issue, we provide a simple approach, called calibrated robust fine-tuning (carot), that incentivizes calibration and robustness on both id and ood datasets. empirical results on imagenet-1k distribution shift evaluation verify the effectiveness of our method.",
        "doi": "",
        "created": "2023-11-03",
        "url": "https://arxiv.org/abs/2311.01723",
        "authors": [
            "changdae oh",
            "mijoo kim",
            "hyesu lim",
            "junhyeok park",
            "euiseog jeong",
            "zhi-qi cheng",
            "kyungwoo song"
        ]
    },
    {
        "id": "2311.05997",
        "title": "jarvis-1: open-world multi-task agents with memory-augmented multimodal   language models",
        "abstract": "achieving human-like planning and control with multimodal observations in an open world is a key milestone for more functional generalist agents. existing approaches can handle certain long-horizon tasks in an open world. however, they still struggle when the number of open-world tasks could potentially be infinite and lack the capability to progressively enhance task completion as game time progresses. we introduce jarvis-1, an open-world agent that can perceive multimodal input (visual observations and human instructions), generate sophisticated plans, and perform embodied control, all within the popular yet challenging open-world minecraft universe. specifically, we develop jarvis-1 on top of pre-trained multimodal language models, which map visual observations and textual instructions to plans. the plans will be ultimately dispatched to the goal-conditioned controllers. we outfit jarvis-1 with a multimodal memory, which facilitates planning using both pre-trained knowledge and its actual game survival experiences. jarvis-1 is the existing most general agent in minecraft, capable of completing over 200 different tasks using control and observation space similar to humans. these tasks range from short-horizon tasks, e.g., \"chopping trees\" to long-horizon tasks, e.g., \"obtaining a diamond pickaxe\". jarvis-1 performs exceptionally well in short-horizon tasks, achieving nearly perfect performance. in the classic long-term task of $\\texttt{obtaindiamondpickaxe}$, jarvis-1 surpasses the reliability of current state-of-the-art agents by 5 times and can successfully complete longer-horizon and more challenging tasks. the project page is available at https://craftjarvis.org/jarvis-1",
        "doi": "",
        "created": "2023-11-10",
        "url": "https://arxiv.org/abs/2311.05997",
        "authors": [
            "zihao wang",
            "shaofei cai",
            "anji liu",
            "yonggang jin",
            "jinbing hou",
            "bowei zhang",
            "haowei lin",
            "zhaofeng he",
            "zilong zheng",
            "yaodong yang",
            "xiaojian ma",
            "yitao liang"
        ]
    },
    {
        "id": "2311.08592",
        "title": "aart: ai-assisted red-teaming with diverse data generation for new   llm-powered applications",
        "abstract": "adversarial testing of large language models (llms) is crucial for their safe and responsible deployment. we introduce a novel approach for automated generation of adversarial evaluation datasets to test the safety of llm generations on new downstream applications. we call it ai-assisted red-teaming (aart) - an automated alternative to current manual red-teaming efforts. aart offers a data generation and augmentation pipeline of reusable and customizable recipes that reduce human effort significantly and enable integration of adversarial testing earlier in new product development. aart generates evaluation datasets with high diversity of content characteristics critical for effective adversarial testing (e.g. sensitive and harmful concepts, specific to a wide range of cultural and geographic regions and application scenarios). the data generation is steered by ai-assisted recipes to define, scope and prioritize diversity within the application context. this feeds into a structured llm-generation process that scales up evaluation priorities. compared to some state-of-the-art tools, aart shows promising results in terms of concept coverage and data quality.",
        "doi": "",
        "created": "2023-11-14",
        "url": "https://arxiv.org/abs/2311.08592",
        "authors": [
            "bhaktipriya radharapu",
            "kevin robinson",
            "lora aroyo",
            "preethi lahoti"
        ]
    },
    {
        "id": "2311.11286",
        "title": "classification of radio galaxies with trainable cosfire filters",
        "abstract": "radio galaxies exhibit a rich diversity of characteristics and emit radio emissions through a variety of radiation mechanisms, making their classification into distinct types based on morphology a complex challenge. to address this challenge effectively, we introduce an innovative approach for radio galaxy classification using cosfire filters. these filters possess the ability to adapt to both the shape and orientation of prototype patterns within images. the cosfire approach is explainable, learning-free, rotation-tolerant, efficient, and does not require a huge training set. to assess the efficacy of our method, we conducted experiments on a benchmark radio galaxy data set comprising of 1180 training samples and 404 test samples. notably, our approach achieved an average accuracy rate of 93.36\\%. this achievement outperforms contemporary deep learning models, and it is the best result ever achieved on this data set. additionally, cosfire filters offer better computational performance, $\\sim$20$\\times$ fewer operations than the densenet-based competing method (when comparing at the same accuracy). our findings underscore the effectiveness of the cosfire filter-based approach in addressing the complexities associated with radio galaxy classification. this research contributes to advancing the field by offering a robust solution that transcends the orientation challenges intrinsic to radio galaxy observations. our method is versatile in that it is applicable to various image classification approaches.",
        "doi": "",
        "created": "2023-11-19",
        "url": "https://arxiv.org/abs/2311.11286",
        "authors": [
            "steven ndungu",
            "trienko grobler",
            "stefan j. wijnholds dimka karastoyanova",
            "george azzopardi"
        ]
    },
    {
        "id": "2311.11810",
        "title": "docpedia: unleashing the power of large multimodal model in the   frequency domain for versatile document understanding",
        "abstract": "this work presents docpedia, a novel large multimodal model (lmm) for versatile ocr-free document understanding, capable of parsing images up to 2,560$\\times$2,560 resolution. unlike existing work either struggle with high-resolution documents or give up the large language model thus vision or language ability constrained, our docpedia directly processes visual input in the frequency domain rather than the pixel space. the unique characteristic enables docpedia to capture a greater amount of visual and textual information using a limited number of visual tokens. to consistently enhance both perception and comprehension abilities of our model, we develop a dual-stage training strategy and enrich instructions/annotations of all training tasks covering multiple document types. extensive quantitative and qualitative experiments conducted on various publicly available benchmarks confirm the mutual benefits of jointly learning perception and comprehension tasks. the results provide further evidence of the effectiveness and superior performance of our docpedia over other methods.",
        "doi": "",
        "created": "2023-11-20",
        "url": "https://arxiv.org/abs/2311.11810",
        "authors": [
            "hao feng",
            "qi liu",
            "hao liu",
            "wengang zhou",
            "houqiang li",
            "can huang"
        ]
    },
    {
        "id": "2311.12188",
        "title": "chatgpt and post-test probability",
        "abstract": "reinforcement learning-based large language models, such as chatgpt, are believed to have potential to aid human experts in many domains, including healthcare. there is, however, little work on chatgpt's ability to perform a key task in healthcare: formal, probabilistic medical diagnostic reasoning. this type of reasoning is used, for example, to update a pre-test probability to a post-test probability. in this work, we probe chatgpt's ability to perform this task. in particular, we ask chatgpt to give examples of how to use bayes rule for medical diagnosis. our prompts range from queries that use terminology from pure probability (e.g., requests for a \"posterior probability\") to queries that use terminology from the medical diagnosis literature (e.g., requests for a \"post-test probability\"). we show how the introduction of medical variable names leads to an increase in the number of errors that chatgpt makes. given our results, we also show how one can use prompt engineering to facilitate chatgpt's partial avoidance of these errors. we discuss our results in light of recent commentaries on sensitivity and specificity. we also discuss how our results might inform new research directions for large language models.",
        "doi": "",
        "created": "2023-11-20",
        "url": "https://arxiv.org/abs/2311.12188",
        "authors": [
            "samuel j. weisenthal"
        ]
    },
    {
        "id": "2311.12825",
        "title": "a pso based method to generate actionable counterfactuals for high   dimensional data",
        "abstract": "counterfactual explanations (cfe) are methods that explain a machine learning model by giving an alternate class prediction of a data point with some minimal changes in its features. it helps the users to identify their data attributes that caused an undesirable prediction like a loan or credit card rejection. we describe an efficient and an actionable counterfactual (cf) generation method based on particle swarm optimization (pso). we propose a simple objective function for the optimization of the instance-centric cf generation problem. the pso brings in a lot of flexibility in terms of carrying out multi-objective optimization in large dimensions, capability for multiple cf generation, and setting box constraints or immutability of data attributes. an algorithm is proposed that incorporates these features and it enables greater control over the proximity and sparsity properties over the generated cfs. the proposed algorithm is evaluated with a set of action-ability metrics in real-world datasets, and the results were superior compared to that of the state-of-the-arts.",
        "doi": "",
        "created": "2023-09-30",
        "url": "https://arxiv.org/abs/2311.12825",
        "authors": [
            "shashank shekhar",
            "asif salim",
            "adesh bansode",
            "vivaswan jinturkar",
            "anirudha nayak"
        ]
    },
    {
        "id": "2311.14656",
        "title": "charting new territories: exploring the geographic and geospatial   capabilities of multimodal llms",
        "abstract": "multimodal large language models (mllms) have shown remarkable capabilities across a broad range of tasks but their knowledge and abilities in the geographic and geospatial domains are yet to be explored, despite potential wide-ranging benefits to navigation, environmental research, urban development, and disaster response. we conduct a series of experiments exploring various vision capabilities of mllms within these domains, particularly focusing on the frontier model gpt-4v, and benchmark its performance against open-source counterparts. our methodology involves challenging these models with a small-scale geographic benchmark consisting of a suite of visual tasks, testing their abilities across a spectrum of complexity. the analysis uncovers not only where such models excel, including instances where they outperform humans, but also where they falter, providing a balanced view of their capabilities in the geographic domain. to enable the comparison and evaluation of future models, our benchmark will be publicly released.",
        "doi": "",
        "created": "2023-11-24",
        "url": "https://arxiv.org/abs/2311.14656",
        "authors": [
            "jonathan roberts",
            "timo l\u00fcddecke",
            "rehan sheikh",
            "kai han",
            "samuel albanie"
        ]
    },
    {
        "id": "2311.14948",
        "title": "effective backdoor mitigation depends on the pre-training objective",
        "abstract": "despite the advanced capabilities of contemporary machine learning (ml) models, they remain vulnerable to adversarial and backdoor attacks. this vulnerability is particularly concerning in real-world deployments, where compromised models may exhibit unpredictable behavior in critical scenarios. such risks are heightened by the prevalent practice of collecting massive, internet-sourced datasets for pre-training multimodal models, as these datasets may harbor backdoors. various techniques have been proposed to mitigate the effects of backdooring in these models such as cleanclip which is the current state-of-the-art approach. in this work, we demonstrate that the efficacy of cleanclip in mitigating backdoors is highly dependent on the particular objective used during model pre-training. we observe that stronger pre-training objectives correlate with harder to remove backdoors behaviors. we show this by training multimodal models on two large datasets consisting of 3 million (cc3m) and 6 million (cc6m) datapoints, under various pre-training objectives, followed by poison removal using cleanclip. we find that cleanclip is ineffective when stronger pre-training objectives are used, even with extensive hyperparameter tuning. our findings underscore critical considerations for ml practitioners who pre-train models using large-scale web-curated data and are concerned about potential backdoor threats. notably, our results suggest that simpler pre-training objectives are more amenable to effective backdoor removal. this insight is pivotal for practitioners seeking to balance the trade-offs between using stronger pre-training objectives and security against backdoor attacks.",
        "doi": "",
        "created": "2023-11-25",
        "url": "https://arxiv.org/abs/2311.14948",
        "authors": [
            "sahil verma",
            "gantavya bhatt",
            "avi schwarzschild",
            "soumye singhal",
            "arnav mohanty das",
            "chirag shah",
            "john p dickerson",
            "jeff bilmes"
        ]
    },
    {
        "id": "2311.16102",
        "title": "diffusion-tta: test-time adaptation of discriminative models via   generative feedback",
        "abstract": "the advancements in generative modeling, particularly the advent of diffusion models, have sparked a fundamental question: how can these models be effectively used for discriminative tasks? in this work, we find that generative models can be great test-time adapters for discriminative models. our method, diffusion-tta, adapts pre-trained discriminative models such as image classifiers, segmenters and depth predictors, to each unlabelled example in the test set using generative feedback from a diffusion model. we achieve this by modulating the conditioning of the diffusion model using the output of the discriminative model. we then maximize the image likelihood objective by backpropagating the gradients to discriminative model's parameters. we show diffusion-tta significantly enhances the accuracy of various large-scale pre-trained discriminative models, such as, imagenet classifiers, clip models, image pixel labellers and image depth predictors. diffusion-tta outperforms existing test-time adaptation methods, including ttt-mae and tent, and particularly shines in online adaptation setups, where the discriminative model is continually adapted to each example in the test set. we provide access to code, results, and visualizations on our website: https://diffusion-tta.github.io/.",
        "doi": "",
        "created": "2023-11-27",
        "url": "https://arxiv.org/abs/2311.16102",
        "authors": [
            "mihir prabhudesai",
            "tsung-wei ke",
            "alexander c. li",
            "deepak pathak",
            "katerina fragkiadaki"
        ]
    },
    {
        "id": "2311.16512",
        "title": "coser: bridging image and language for cognitive super-resolution",
        "abstract": "existing super-resolution (sr) models primarily focus on restoring local texture details, often neglecting the global semantic information within the scene. this oversight can lead to the omission of crucial semantic details or the introduction of inaccurate textures during the recovery process. in our work, we introduce the cognitive super-resolution (coser) framework, empowering sr models with the capacity to comprehend low-resolution images. we achieve this by marrying image appearance and language understanding to generate a cognitive embedding, which not only activates prior information from large text-to-image diffusion models but also facilitates the generation of high-quality reference images to optimize the sr process. to further improve image fidelity, we propose a novel condition injection scheme called \"all-in-attention\", consolidating all conditional information into a single module. consequently, our method successfully restores semantically correct and photorealistic details, demonstrating state-of-the-art performance across multiple benchmarks. code: https://github.com/vinhyu/coser",
        "doi": "",
        "created": "2023-11-27",
        "url": "https://arxiv.org/abs/2311.16512",
        "authors": [
            "haoze sun",
            "wenbo li",
            "jianzhuang liu",
            "haoyu chen",
            "renjing pei",
            "xueyi zou",
            "youliang yan",
            "yujiu yang"
        ]
    },
    {
        "id": "2311.16605",
        "title": "lastgl: an industrial framework for large-scale temporal graph learning",
        "abstract": "over the past few years, graph neural networks (gnns) have become powerful and practical tools for learning on (static) graph-structure data. however, many real-world applications, such as social networks and e-commerce, involve temporal graphs where nodes and edges are dynamically evolving. temporal graph neural networks (tgnns) have progressively emerged as an extension of gnns to address time-evolving graphs and have gradually become a trending research topic in both academics and industry. advancing research and application in such an emerging field necessitates the development of new tools to compose tgnn models and unify their different schemes for dealing with temporal graphs. in this work, we introduce lastgl, an industrial framework that integrates unified and extensible implementations of common temporal graph learning algorithms for various advanced tasks. the purpose of lastgl is to provide the essential building blocks for solving temporal graph learning tasks, focusing on the guiding principles of user-friendliness and quick prototyping on which pytorch is based. in particular, lastgl provides comprehensive temporal graph datasets, tgnn models and utilities along with well-documented tutorials, making it suitable for both absolute beginners and expert deep learning practitioners alike.",
        "doi": "",
        "created": "2023-11-28",
        "url": "https://arxiv.org/abs/2311.16605",
        "authors": [
            "jintang li",
            "jiawang dan",
            "ruofan wu",
            "jing zhou",
            "sheng tian",
            "yunfei liu",
            "baokun wang",
            "changhua meng",
            "weiqiang wang",
            "yuchang zhu",
            "liang chen",
            "zibin zheng"
        ]
    },
    {
        "id": "2311.16733",
        "title": "llms for science: usage for code generation and data analysis",
        "abstract": "large language models (llms) have been touted to enable increased productivity in many areas of today's work life. scientific research as an area of work is no exception: the potential of llm-based tools to assist in the daily work of scientists has become a highly discussed topic across disciplines. however, we are only at the very onset of this subject of study. it is still unclear how the potential of llms will materialise in research practice. with this study, we give first empirical evidence on the use of llms in the research process. we have investigated a set of use cases for llm-based tools in scientific research, and conducted a first study to assess to which degree current tools are helpful. in this paper we report specifically on use cases related to software engineering, such as generating application code and developing scripts for data analytics. while we studied seemingly simple use cases, results across tools differ significantly. our results highlight the promise of llm-based tools in general, yet we also observe various issues, particularly regarding the integrity of the output these tools provide.",
        "doi": "",
        "created": "2023-11-28",
        "url": "https://arxiv.org/abs/2311.16733",
        "authors": [
            "mohamed nejjar",
            "luca zacharias",
            "fabian stiehle",
            "ingo weber"
        ]
    },
    {
        "id": "2311.16867",
        "title": "the falcon series of open language models",
        "abstract": "we introduce the falcon series: 7b, 40b, and 180b parameters causal decoder-only models trained on a diverse high-quality corpora predominantly assembled from web data. the largest model, falcon-180b, has been trained on over 3.5 trillion tokens of text--the largest openly documented pretraining run. falcon-180b significantly outperforms models such as palm or chinchilla, and improves upon concurrently developed models such as llama 2 or inflection-1. it nears the performance of palm-2-large at a reduced pretraining and inference cost, making it, to our knowledge, one of the three best language models in the world along with gpt-4 and palm-2-large. we report detailed evaluations, as well as a deep dive into the methods and custom tooling employed to pretrain falcon. notably, we report on our custom distributed training codebase, allowing us to efficiently pretrain these models on up to 4,096 a100s on cloud aws infrastructure with limited interconnect. we release a 600b tokens extract of our web dataset, as well as the falcon-7/40/180b models under a permissive license to foster open-science and accelerate the development of an open ecosystem of large language models.",
        "doi": "",
        "created": "2023-11-28",
        "url": "https://arxiv.org/abs/2311.16867",
        "authors": [
            "ebtesam almazrouei",
            "hamza alobeidli",
            "abdulaziz alshamsi",
            "alessandro cappelli",
            "ruxandra cojocaru",
            "m\u00e9rouane debbah",
            "\u00e9tienne goffinet",
            "daniel hesslow",
            "julien launay",
            "quentin malartic",
            "daniele mazzotta",
            "badreddine noune",
            "baptiste pannier",
            "guilherme penedo"
        ]
    },
    {
        "id": "2311.17431",
        "title": "grounding foundation models through federated transfer learning: a   general framework",
        "abstract": "foundation models (fms) such as gpt-4 encoded with vast knowledge and powerful emergent abilities have achieved remarkable success in various natural language processing and computer vision tasks. grounding fms by adapting them to domain-specific tasks or augmenting them with domain-specific knowledge enables us to exploit the full potential of fms. however, grounding fms faces several challenges, stemming primarily from constrained computing resources, data privacy, model heterogeneity, and model ownership. federated transfer learning (ftl), the combination of federated learning and transfer learning, provides promising solutions to address these challenges. in recent years, the need for grounding fms leveraging ftl, coined ftl-fm, has arisen strongly in both academia and industry. motivated by the strong growth in ftl-fm research and the potential impact of ftl-fm on industrial applications, we propose an ftl-fm framework that formulates problems of grounding fms in the federated learning setting, construct a detailed taxonomy based on the ftl-fm framework to categorize state-of-the-art ftl-fm works, and comprehensively overview ftl-fm works based on the proposed taxonomy. we also establish correspondences between ftl-fm and conventional phases of adapting fm so that fm practitioners can align their research works with ftl-fm. in addition, we overview advanced efficiency-improving and privacy-preserving techniques because efficiency and privacy are critical concerns in ftl-fm. last, we discuss opportunities and future research directions of ftl-fm.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.17431",
        "authors": [
            "yan kang",
            "tao fan",
            "hanlin gu",
            "lixin fan",
            "qiang yang"
        ]
    },
    {
        "id": "2311.17438",
        "title": "clomo: counterfactual logical modification with large language models",
        "abstract": "in this study, we delve into the realm of counterfactual reasoning capabilities of large language models (llms). our primary objective is to cultivate the counterfactual thought processes within llms and rigorously assess these processes for their validity. specifically, we introduce a novel task, counterfactual logical modification (clomo), and a high-quality human-annotated benchmark. in this task, llms must adeptly alter a given argumentative text to uphold a predetermined logical relationship. to effectively evaluate a generation model's counterfactual capabilities, we propose an innovative evaluation metric, the logicaware counterfactual score to directly evaluate the natural language output of llms instead of modeling the task as a multiple-choice problem. analysis shows that the proposed automatic metric aligns well with human preference. our experimental results show that while llms demonstrate a notable capacity for logical counterfactual thinking, there remains a discernible gap between their current abilities and human performance.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.17438",
        "authors": [
            "yinya huang",
            "ruixin hong",
            "hongming zhang",
            "wei shao",
            "zhicheng yang",
            "dong yu",
            "changshui zhang",
            "xiaodan liang",
            "linqi song"
        ]
    },
    {
        "id": "2311.17943",
        "title": "layercollapse: adaptive compression of neural networks",
        "abstract": "handling the ever-increasing scale of contemporary deep learning and transformer-based models poses a significant challenge. although great strides have been made in optimizing model compression techniques such as model architecture search and knowledge distillation, the availability of data and computational resources remains a considerable hurdle for these optimizations. this paper introduces layercollapse, a novel alternative adaptive model compression methodology. layercollapse works by eliminating non-linearities within the network and collapsing two consecutive fully connected layers into a single linear transformation. this approach simultaneously reduces both the number of layers and the parameter count, thereby enhancing model efficiency. we also introduce a compression aware regularizer, which compresses the model in alignment with the dataset quality and model expressiveness, consequently reducing overfitting across tasks. our results demonstrate layercollapse's effective compression and regularization capabilities in multiple fine-grained classification benchmarks, achieving up to 74% post training compression with minimal accuracy loss. we compare this method with knowledge distillation on the same target network, showcasing a five-fold increase in computational efficiency and 8% improvement in overall accuracy on the imagenet dataset.",
        "doi": "",
        "created": "2023-11-28",
        "url": "https://arxiv.org/abs/2311.17943",
        "authors": [
            "soheil zibakhsh shabgahi",
            "mohammad soheil shariff",
            "farinaz koushanfar"
        ]
    },
    {
        "id": "2311.17946",
        "title": "dreamsync: aligning text-to-image generation with image understanding   feedback",
        "abstract": "despite their wide-spread success, text-to-image models (t2i) still struggle to produce images that are both aesthetically pleasing and faithful to the user's input text. we introduce dreamsync, a model-agnostic training algorithm by design that improves t2i models to be faithful to the text input. dreamsync builds off a recent insight from tifa's evaluation framework -- that large vision-language models (vlms) can effectively identify the fine-grained discrepancies between generated images and the text inputs. dreamsync uses this insight to train t2i models without any labeled data; it improves t2i models using its own generations. first, it prompts the model to generate several candidate images for a given input text. then, it uses two vlms to select the best generation: a visual question answering model that measures the alignment of generated images to the text, and another that measures the generation's aesthetic quality. after selection, we use lora to iteratively finetune the t2i model to guide its generation towards the selected best generations. dreamsync does not need any additional human annotation. model architecture changes, or reinforcement learning. despite its simplicity, dreamsync improves both the semantic alignment and aesthetic appeal of two diffusion-based t2i models, evidenced by multiple benchmarks (+1.7% on tifa, +2.9% on dsg1k, +3.4% on vila aesthetic) and human evaluation.",
        "doi": "",
        "created": "2023-11-28",
        "url": "https://arxiv.org/abs/2311.17946",
        "authors": [
            "jiao sun",
            "deqing fu",
            "yushi hu",
            "su wang",
            "royi rassin",
            "da-cheng juan",
            "dana alon",
            "charles herrmann",
            "sjoerd van steenkiste",
            "ranjay krishna",
            "cyrus rashtchian"
        ]
    },
    {
        "id": "2311.17950",
        "title": "generalized large-scale data condensation via various backbone and   statistical matching",
        "abstract": "the lightweight \"local-match-global\" matching introduced by sre2l successfully creates a distilled dataset with comprehensive information on the full 224x224 imagenet-1k. however, this one-sided approach is limited to a particular backbone, layer, and statistics, which limits the improvement of the generalization of a distilled dataset. we suggest that sufficient and various \"local-match-global\" matching are more precise and effective than a single one and has the ability to create a distilled dataset with richer information and better generalization. we call this perspective \"generalized matching\" and propose generalized various backbone and statistical matching (g-vbsm) in this work, which aims to create a synthetic dataset with densities, ensuring consistency with the complete dataset across various backbones, layers, and statistics. as experimentally demonstrated, g-vbsm is the first algorithm to obtain strong performance across both small-scale and large-scale datasets. specifically, g-vbsm achieves a performance of 38.7% on cifar-100 with 128-width convnet, 47.6% on tiny-imagenet with resnet18, and 31.4% on the full 224x224 imagenet-1k with resnet18, under images per class (ipc) 10, 50, and 10, respectively. these results surpass all sota methods by margins of 3.9%, 6.5%, and 10.1%, respectively.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.17950",
        "authors": [
            "shitong shao",
            "zeyuan yin",
            "muxin zhou",
            "xindong zhang",
            "zhiqiang shen"
        ]
    },
    {
        "id": "2311.17958",
        "title": "communityai: towards community-based federated learning",
        "abstract": "federated learning (fl) has emerged as a promising paradigm to train machine learning models collaboratively while preserving data privacy. however, its widespread adoption faces several challenges, including scalability, heterogeneous data and devices, resource constraints, and security concerns. despite its promise, fl has not been specifically adapted for community domains, primarily due to the wide-ranging differences in data types and context, devices and operational conditions, environmental factors, and stakeholders. in response to these challenges, we present a novel framework for community-based federated learning called communityai. communityai enables participants to be organized into communities based on their shared interests, expertise, or data characteristics. community participants collectively contribute to training and refining learning models while maintaining data and participant privacy within their respective groups. within this paper, we discuss the conceptual architecture, system requirements, processes, and future challenges that must be solved. finally, our goal within this paper is to present our vision regarding enabling a collaborative learning process within various communities.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.17958",
        "authors": [
            "ilir murturi",
            "praveen kumar donta",
            "schahram dustdar"
        ]
    },
    {
        "id": "2311.17959",
        "title": "transformer based model for predicting rapid impact compaction outcomes:   a case study of utapao international airport",
        "abstract": "this paper introduces a novel deep learning approach to predict the engineering properties of the ground improved by rapid impact compaction (ric), which is a ground improvement technique that uses a drop hammer to compact the soil and fill layers. the proposed approach uses transformer-based neural networks to capture the complex nonlinear relationships between the input features, such as the hammer energy, drop height, and number of blows, and the output variables, such as the cone resistance. the approach is applied to a real-world dataset from a trial test section for the new apron construction of the utapao international airport in thailand. the results show that the proposed approach outperforms the existing methods in terms of prediction accuracy and efficiency and provides interpretable attention maps that reveal the importance of different features for ric prediction. the paper also discusses the limitations and future directions of applying deep learning methods to ric prediction.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.17959",
        "authors": [
            "sompote youwai",
            "sirasak detcheewa"
        ]
    },
    {
        "id": "2311.17961",
        "title": "skilful precipitation nowcasting using nowcastnet",
        "abstract": "designing early warning system for precipitation requires accurate short-term forecasting system. climate change has led to an increase in frequency of extreme weather events, and hence such systems can prevent disasters and loss of life. managing such events remain a challenge for both public and private institutions. precipitation nowcasting can help relevant institutions to better prepare for such events as they impact agriculture, transport, public health and safety, etc. physics-based numerical weather prediction (nwp) is unable to perform well for nowcasting because of large computational turn-around time. deep-learning based models on the other hand are able to give predictions within seconds. we use recently proposed nowcastnet, a physics-conditioned deep generative network, to forecast precipitation for different regions of europe using satellite images. both spatial and temporal transfer learning is done by forecasting for the unseen regions and year. model makes realistic predictions and is able to outperform baseline for such a prediction task.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.17961",
        "authors": [
            "ajitabh kumar"
        ]
    },
    {
        "id": "2311.17968",
        "title": "latent alignment with deep set eeg decoders",
        "abstract": "the variability in eeg signals between different individuals poses a significant challenge when implementing brain-computer interfaces (bci). commonly proposed solutions to this problem include deep learning models, due to their increased capacity and generalization, as well as explicit domain adaptation techniques. here, we introduce the latent alignment method that won the benchmarks for eeg transfer learning (beetl) competition and present its formulation as a deep set applied on the set of trials from a given subject. its performance is compared to recent statistical domain adaptation techniques under various conditions. the experimental paradigms include motor imagery (mi), oddball event-related potentials (erp) and sleep stage classification, where different well-established deep learning models are applied on each task. our experimental results show that performing statistical distribution alignment at later stages in a deep learning model is beneficial to the classification accuracy, yielding the highest performance for our proposed method. we further investigate practical considerations that arise in the context of using deep learning and statistical alignment for eeg decoding. in this regard, we study class-discriminative artifacts that can spuriously improve results for deep learning models, as well as the impact of class-imbalance on alignment. we delineate a trade-off relationship between increased classification accuracy when alignment is performed at later modeling stages, and susceptibility to class-imbalance in the set of trials that the statistics are computed on.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.17968",
        "authors": [
            "stylianos bakas",
            "siegfried ludwig",
            "dimitrios a. adamos",
            "nikolaos laskaris",
            "yannis panagakis",
            "stefanos zafeiriou"
        ]
    },
    {
        "id": "2311.17973",
        "title": "homogeneous artificial neural network",
        "abstract": "the paper proposes an artificial neural network (ann) being a global approximator for a special class of functions, which are known as generalized homogeneous. the homogeneity means a symmetry of a function with respect to a group of transformations having topological characterization of a dilation. in this paper, a class of the so-called linear dilations is considered. a homogeneous universal approximation theorem is proven. procedures for an upgrade of an existing ann to a homogeneous one are developed. theoretical results are supported by examples from the various domains (computer science, systems theory and automatic control).",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.17973",
        "authors": [
            "andrey polyakov"
        ]
    },
    {
        "id": "2311.17983",
        "title": "improving faithfulness for vision transformers",
        "abstract": "vision transformers (vits) have achieved state-of-the-art performance for various vision tasks. one reason behind the success lies in their ability to provide plausible innate explanations for the behavior of neural architectures. however, vits suffer from issues with explanation faithfulness, as their focal points are fragile to adversarial attacks and can be easily changed with even slight perturbations on the input image. in this paper, we propose a rigorous approach to mitigate these issues by introducing faithful vits (fvits). briefly speaking, an fvit should have the following two properties: (1) the top-$k$ indices of its self-attention vector should remain mostly unchanged under input perturbation, indicating stable explanations; (2) the prediction distribution should be robust to perturbations. to achieve this, we propose a new method called denoised diffusion smoothing (dds), which adopts randomized smoothing and diffusion-based denoising. we theoretically prove that processing vits directly with dds can turn them into fvits. we also show that gaussian noise is nearly optimal for both $\\ell_2$ and $\\ell_\\infty$-norm cases. finally, we demonstrate the effectiveness of our approach through comprehensive experiments and evaluations. specifically, we compare our fvits with other baselines through visual interpretation and robustness accuracy under adversarial attacks. results show that fvits are more robust against adversarial attacks while maintaining the explainability of attention, indicating higher faithfulness.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.17983",
        "authors": [
            "lijie hu",
            "yixin liu",
            "ninghao liu",
            "mengdi huai",
            "lichao sun",
            "di wang"
        ]
    },
    {
        "id": "2311.18021",
        "title": "understanding and improving in-context learning on vision-language   models",
        "abstract": "recently, in-context learning (icl) on large language models (llms) has received great attention, and this technique can also be applied to vision-language models (vlms) built upon llms. these vlms can respond to queries by conditioning responses on a series of multimodal demonstrations, which comprise images, queries, and answers. though icl has been extensively studied on llms, its research on vlms remains limited. the inclusion of additional visual information in the demonstrations motivates the following research questions: which of the two modalities in the demonstration is more significant? how can we select effective multimodal demonstrations to enhance icl performance? this study investigates the significance of both visual and language information. our findings indicate that icl in vlms is predominantly driven by the textual information in the demonstrations whereas the visual information in the demonstrations barely affects the icl performance. subsequently, we provide an understanding of the findings by analyzing the model information flow and comparing model inner states given different icl settings. motivated by our analysis, we propose a simple yet effective approach, termed mixed modality in-context example selection (mmices), which considers both visual and language modalities when selecting demonstrations and shows better icl performance. extensive experiments are conducted to support our findings, understanding, and improvement of the icl performance of vlms.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.18021",
        "authors": [
            "shuo chen",
            "zhen han",
            "bailan he",
            "mark buckley",
            "philip torr",
            "volker tresp",
            "jindong gu"
        ]
    },
    {
        "id": "2311.18022",
        "title": "a trainable manifold for accurate approximation with relu networks",
        "abstract": "we present a novel technique for exercising greater control of the weights of relu activated neural networks to produce more accurate function approximations. many theoretical works encode complex operations into relu networks using smaller base components. in these works, a common base component is a constant width approximation to x^2, which has exponentially decaying error with respect to depth. we extend this block to represent a greater range of convex one-dimensional functions. we derive a manifold of weights such that the output of these new networks utilizes exponentially many piecewise-linear segments. this manifold guides their training process to overcome drawbacks associated with random initialization and unassisted gradient descent. we train these networks to approximate functions which do not necessarily lie on the manifold, showing a significant reduction of error values over conventional approaches.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.18022",
        "authors": [
            "max milkert",
            "forrest laine"
        ]
    },
    {
        "id": "2311.18028",
        "title": "filtered semi-markov crf",
        "abstract": "semi-markov crf has been proposed as an alternative to the traditional linear chain crf for text segmentation tasks such as named entity recognition (ner). unlike crf, which treats text segmentation as token-level prediction, semi-crf considers segments as the basic unit, making it more expressive. however, semi-crf suffers from two major drawbacks: (1) quadratic complexity over sequence length, as it operates on every span of the input sequence, and (2) inferior performance compared to crf for sequence labeling tasks like ner. in this paper, we introduce filtered semi-markov crf, a variant of semi-crf that addresses these issues by incorporating a filtering step to eliminate irrelevant segments, reducing complexity and search space. our approach is evaluated on several ner benchmarks, where it outperforms both crf and semi-crf while being significantly faster. the implementation of our method is available on \\href{https://github.com/urchade/filtered-semi-markov-crf}{github}.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.18028",
        "authors": [
            "urchade zaratiana",
            "nadi tomeh",
            "niama el khbir",
            "pierre holat",
            "thierry charnois"
        ]
    },
    {
        "id": "2311.18029",
        "title": "a bag of receptive fields for time series extrinsic predictions",
        "abstract": "high-dimensional time series data poses challenges due to its dynamic nature, varying lengths, and presence of missing values. this kind of data requires extensive preprocessing, limiting the applicability of existing time series classification and time series extrinsic regression techniques. for this reason, we propose borf, a bag-of-receptive-fields model, which incorporates notions from time series convolution and 1d-sax to handle univariate and multivariate time series with varying lengths and missing values. we evaluate borf on time series classification and time series extrinsic regression tasks using the full uea and ucr repositories, demonstrating its competitive performance against state-of-the-art methods. finally, we outline how this representation can naturally provide saliency and feature-based explanations.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.18029",
        "authors": [
            "francesco spinnato",
            "riccardo guidotti",
            "anna monreale",
            "mirco nanni"
        ]
    },
    {
        "id": "2311.18046",
        "title": "making data work count",
        "abstract": "in this paper, we examine the work of data annotation. specifically, we focus on the role of counting or quantification in organising annotation work. based on an ethnographic study of data annotation in two outsourcing centres in india, we observe that counting practices and its associated logics are an integral part of day-to-day annotation activities. in particular, we call attention to the presumption of total countability observed in annotation - the notion that everything, from tasks, datasets and deliverables, to workers, work time, quality and performance, can be managed by applying the logics of counting. to examine this, we draw on sociological and socio-technical scholarship on quantification and develop the lens of a 'regime of counting' that makes explicit the specific counts, practices, actors and structures that underpin the pervasive counting in annotation. we find that within the ai supply chain and data work, counting regimes aid the assertion of authority by the ai clients (also called requesters) over annotation processes, constituting them as reductive, standardised, and homogenous. we illustrate how this has implications for i) how annotation work and workers get valued, ii) the role human discretion plays in annotation, and iii) broader efforts to introduce accountable and more just practices in ai. through these implications, we illustrate the limits of operating within the logic of total countability. instead, we argue for a view of counting as partial - located in distinct geographies, shaped by specific interests and accountable in only limited ways. this, we propose, sets the stage for a fundamentally different orientation to counting and what counts in data annotation.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.18046",
        "authors": [
            "srravya chandhiramowuli",
            "alex taylor",
            "sara heitlinger",
            "ding wang"
        ]
    },
    {
        "id": "2311.18054",
        "title": "i know you did not write that! a sampling based watermarking method for   identifying machine generated text",
        "abstract": "potential harms of large language models such as mass misinformation and plagiarism can be partially mitigated if there exists a reliable way to detect machine generated text. in this paper, we propose a new watermarking method to detect machine-generated texts. our method embeds a unique pattern within the generated text, ensuring that while the content remains coherent and natural to human readers, it carries distinct markers that can be identified algorithmically. specifically, we intervene with the token sampling process in a way which enables us to trace back our token choices during the detection phase. we show how watermarking affects textual quality and compare our proposed method with a state-of-the-art watermarking method in terms of robustness and detectability. through extensive experiments, we demonstrate the effectiveness of our watermarking scheme in distinguishing between watermarked and non-watermarked text, achieving high detection rates while maintaining textual quality.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.18054",
        "authors": [
            "kaan efe kele\u015f",
            "\u00f6mer kaan g\u00fcrb\u00fcz",
            "mucahid kutlu"
        ]
    },
    {
        "id": "2311.18062",
        "title": "understanding your agent: leveraging large language models for behavior   explanation",
        "abstract": "intelligent agents such as robots are increasingly deployed in real-world, safety-critical settings. it is vital that these agents are able to explain the reasoning behind their decisions to human counterparts; however, their behavior is often produced by uninterpretable models such as deep neural networks. we propose an approach to generate natural language explanations for an agent's behavior based only on observations of states and actions, thus making our method independent from the underlying model's representation. for such models, we first learn a behavior representation and subsequently use it to produce plausible explanations with minimal hallucination while affording user interaction with a pre-trained large language model. we evaluate our method in a multi-agent search-and-rescue environment and demonstrate the effectiveness of our explanations for agents executing various behaviors. through user studies and empirical experiments, we show that our approach generates explanations as helpful as those produced by a human domain expert while enabling beneficial interactions such as clarification and counterfactual queries.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.18062",
        "authors": [
            "xijia zhang",
            "yue guo",
            "simon stepputtis",
            "katia sycara",
            "joseph campbell"
        ]
    },
    {
        "id": "2311.18094",
        "title": "self-driving telescopes: autonomous scheduling of astronomical   observation campaigns with offline reinforcement learning",
        "abstract": "modern astronomical experiments are designed to achieve multiple scientific goals, from studies of galaxy evolution to cosmic acceleration. these goals require data of many different classes of night-sky objects, each of which has a particular set of observational needs. these observational needs are typically in strong competition with one another. this poses a challenging multi-objective optimization problem that remains unsolved. the effectiveness of reinforcement learning (rl) as a valuable paradigm for training autonomous systems has been well-demonstrated, and it may provide the basis for self-driving telescopes capable of optimizing the scheduling for astronomy campaigns. simulated datasets containing examples of interactions between a telescope and a discrete set of sky locations on the celestial sphere can be used to train an rl model to sequentially gather data from these several locations to maximize a cumulative reward as a measure of the quality of the data gathered. we use simulated data to test and compare multiple implementations of a deep q-network (dqn) for the task of optimizing the schedule of observations from the stone edge observatory (seo). we combine multiple improvements on the dqn and adjustments to the dataset, showing that dqns can achieve an average reward of 87%+-6% of the maximum achievable reward in each state on the test set. this is the first comparison of offline rl algorithms for a particular astronomical challenge and the first open-source framework for performing such a comparison and assessment task.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.18094",
        "authors": [
            "franco terranova",
            "m. voetberg",
            "brian nord",
            "amanda pagul"
        ]
    },
    {
        "id": "2311.18102",
        "title": "patchbmi-net: lightweight facial patch-based ensemble for bmi prediction",
        "abstract": "due to an alarming trend related to obesity affecting 93.3 million adults in the united states alone, body mass index (bmi) and body weight have drawn significant interest in various health monitoring applications. consequently, several studies have proposed self-diagnostic facial image-based bmi prediction methods for healthy weight monitoring. these methods have mostly used convolutional neural network (cnn) based regression baselines, such as vgg19, resnet50, and efficient-netb0, for bmi prediction from facial images. however, the high computational requirement of these heavy-weight cnn models limits their deployment to resource-constrained mobile devices, thus deterring weight monitoring using smartphones. this paper aims to develop a lightweight facial patch-based ensemble (patchbmi-net) for bmi prediction to facilitate the deployment and weight monitoring using smartphones. extensive experiments on bmi-annotated facial image datasets suggest that our proposed patchbmi-net model can obtain mean absolute error (mae) in the range [3.58, 6.51] with a size of about 3.3 million parameters. on cross-comparison with heavyweight models, such as resnet-50 and xception, trained for bmi prediction from facial images, our proposed patchbmi-net obtains equivalent mae along with the model size reduction of about 5.4x and the average inference time reduction of about 3x when deployed on apple-14 smartphone. thus, demonstrating performance efficiency as well as low latency for on-device deployment and weight monitoring using smartphone applications.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.18102",
        "authors": [
            "parshuram n. aarotale",
            "twyla hill",
            "ajita rattani"
        ]
    },
    {
        "id": "2311.18114",
        "title": "composition of nondeterministic and stochastic services for ltlf task   specifications",
        "abstract": "in this paper, we study the composition of services so as to obtain runs satisfying a task specification in linear temporal logic on finite traces (ltlf). we study the problem in the case services are nondeterministic and the ltlf specification can be exactly met, and in the case services are stochastic, where we are interested in maximizing the probability of satisfaction of the ltlf specification and, simultaneously, minimizing the utilization cost of the services. to do so, we combine techniques from ltlf synthesis, service composition \\`a la roman model, reactive synthesis, and bi-objective lexicographic optimization on mdps. this framework has several interesting applications, including smart manufacturing and digital twins.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.18114",
        "authors": [
            "giuseppe de giacomo",
            "marco favorito",
            "luciana silo"
        ]
    },
    {
        "id": "2311.18138",
        "title": "algorithmic persuasion through simulation: information design in the age   of generative ai",
        "abstract": "how can an informed sender persuade a receiver, having only limited information about the receiver's beliefs? motivated by research showing generative ai can simulate economic agents, we initiate the study of information design with an oracle. we assume the sender can learn more about the receiver by querying this oracle, e.g., by simulating the receiver's behavior. aside from ai motivations such as general-purpose large language models (llms) and problem-specific machine learning models, alternate motivations include customer surveys and querying a small pool of live users.   specifically, we study bayesian persuasion where the sender has a second-order prior over the receiver's beliefs. after a fixed number of queries to an oracle to refine this prior, the sender commits to an information structure. upon receiving the message, the receiver takes a payoff-relevant action maximizing her expected utility given her posterior beliefs. we design polynomial-time querying algorithms that optimize the sender's expected utility in this bayesian persuasion game. as a technical contribution, we show that queries form partitions of the space of receiver beliefs that can be used to quantify the sender's knowledge.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.18138",
        "authors": [
            "keegan harris",
            "nicole immorlica",
            "brendan lucier",
            "aleksandrs slivkins"
        ]
    },
    {
        "id": "2311.18190",
        "title": "toward the tradeoffs between privacy, fairness and utility in federated   learning",
        "abstract": "federated learning (fl) is a novel privacy-protection distributed machine learning paradigm that guarantees user privacy and prevents the risk of data leakage due to the advantage of the client's local training. researchers have struggled to design fair fl systems that ensure fairness of results. however, the interplay between fairness and privacy has been less studied. increasing the fairness of fl systems can have an impact on user privacy, while an increase in user privacy can affect fairness. in this work, on the client side, we use fairness metrics, such as demographic parity (demp), equalized odds (eos), and disparate impact (di), to construct the local fair model. to protect the privacy of the client model, we propose a privacy-protection fairness fl method. the results show that the accuracy of the fair model with privacy increases because privacy breaks the constraints of the fairness metrics. in our experiments, we conclude the relationship between privacy, fairness and utility, and there is a tradeoff between these.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.18190",
        "authors": [
            "kangkang sun",
            "xiaojin zhang",
            "xi lin",
            "gaolei li",
            "jing wang",
            "jianhua li"
        ]
    },
    {
        "id": "2311.18206",
        "title": "scope-rl: a python library for offline reinforcement learning and   off-policy evaluation",
        "abstract": "this paper introduces scope-rl, a comprehensive open-source python software designed for offline reinforcement learning (offline rl), off-policy evaluation (ope), and selection (ops). unlike most existing libraries that focus solely on either policy learning or evaluation, scope-rl seamlessly integrates these two key aspects, facilitating flexible and complete implementations of both offline rl and ope processes. scope-rl put particular emphasis on its ope modules, offering a range of ope estimators and robust evaluation-of-ope protocols. this approach enables more in-depth and reliable ope compared to other packages. for instance, scope-rl enhances ope by estimating the entire reward distribution under a policy rather than its mere point-wise expected value. additionally, scope-rl provides a more thorough evaluation-of-ope by presenting the risk-return tradeoff in ope results, extending beyond mere accuracy evaluations in existing ope literature. scope-rl is designed with user accessibility in mind. its user-friendly apis, comprehensive documentation, and a variety of easy-to-follow examples assist researchers and practitioners in efficiently implementing and experimenting with various offline rl methods and ope estimators, tailored to their specific problem contexts. the documentation of scope-rl is available at https://scope-rl.readthedocs.io/en/latest/.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.18206",
        "authors": [
            "haruka kiyohara",
            "ren kishimoto",
            "kosuke kawakami",
            "ken kobayashi",
            "kazuhide nakata",
            "yuta saito"
        ]
    },
    {
        "id": "2311.18207",
        "title": "towards assessing and benchmarking risk-return tradeoff of off-policy   evaluation",
        "abstract": "off-policy evaluation (ope) aims to assess the effectiveness of counterfactual policies using only offline logged data and is often used to identify the top-k promising policies for deployment in online a/b tests. existing evaluation metrics for ope estimators primarily focus on the \"accuracy\" of ope or that of downstream policy selection, neglecting risk-return tradeoff in the subsequent online policy deployment. to address this issue, we draw inspiration from portfolio evaluation in finance and develop a new metric, called sharperatio@k, which measures the risk-return tradeoff of policy portfolios formed by an ope estimator under varying online evaluation budgets (k). we validate our metric in two example scenarios, demonstrating its ability to effectively distinguish between low-risk and high-risk estimators and to accurately identify the most efficient estimator. this efficient estimator is characterized by its capability to form the most advantageous policy portfolios, maximizing returns while minimizing risks during online deployment, a nuance that existing metrics typically overlook. to facilitate a quick, accurate, and consistent evaluation of ope via sharperatio@k, we have also integrated this metric into an open-source software, scope-rl. employing sharperatio@k and scope-rl, we conduct comprehensive benchmarking experiments on various estimators and rl tasks, focusing on their risk-return tradeoff. these experiments offer several interesting directions and suggestions for future ope research.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.18207",
        "authors": [
            "haruka kiyohara",
            "ren kishimoto",
            "kosuke kawakami",
            "ken kobayashi",
            "kazuhide nakata",
            "yuta saito"
        ]
    },
    {
        "id": "2311.18213",
        "title": "beyond two-tower matching: learning sparse retrievable   cross-interactions for recommendation",
        "abstract": "two-tower models are a prevalent matching framework for recommendation, which have been widely deployed in industrial applications. the success of two-tower matching attributes to its efficiency in retrieval among a large number of items, since the item tower can be precomputed and used for fast approximate nearest neighbor (ann) search. however, it suffers two main challenges, including limited feature interaction capability and reduced accuracy in online serving. existing approaches attempt to design novel late interactions instead of dot products, but they still fail to support complex feature interactions or lose retrieval efficiency. to address these challenges, we propose a new matching paradigm named sparcode, which supports not only sophisticated feature interactions but also efficient retrieval. specifically, sparcode introduces an all-to-all interaction module to model fine-grained query-item interactions. besides, we design a discrete code-based sparse inverted index jointly trained with the model to achieve effective and efficient model inference. extensive experiments have been conducted on open benchmark datasets to demonstrate the superiority of our framework. the results show that sparcode significantly improves the accuracy of candidate item matching while retaining the same level of retrieval efficiency with two-tower models. our source code will be available at mindspore/models.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.18213",
        "authors": [
            "liangcai su",
            "fan yan",
            "jieming zhu",
            "xi xiao",
            "haoyi duan",
            "zhou zhao",
            "zhenhua dong",
            "ruiming tang"
        ]
    },
    {
        "id": "2311.18224",
        "title": "reasoning with the theory of mind for pragmatic semantic communication",
        "abstract": "in this paper, a pragmatic semantic communication framework that enables effective goal-oriented information sharing between two-intelligent agents is proposed. in particular, semantics is defined as the causal state that encapsulates the fundamental causal relationships and dependencies among different features extracted from data. the proposed framework leverages the emerging concept in machine learning (ml) called theory of mind (tom). it employs a dynamic two-level (wireless and semantic) feedback mechanism to continuously fine-tune neural network components at the transmitter. thanks to the tom, the transmitter mimics the actual mental state of the receiver's reasoning neural network operating semantic interpretation. then, the estimated mental state at the receiver is dynamically updated thanks to the proposed dynamic two-level feedback mechanism. at the lower level, conventional channel quality metrics are used to optimize the channel encoding process based on the wireless communication channel's quality, ensuring an efficient mapping of semantic representations to a finite constellation. additionally, a semantic feedback level is introduced, providing information on the receiver's perceived semantic effectiveness with minimal overhead. numerical evaluations demonstrate the framework's ability to achieve efficient communication with a reduced amount of bits while maintaining the same semantics, outperforming conventional systems that do not exploit the tom-based reasoning.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.18224",
        "authors": [
            "christo kurisummoottil thomas",
            "emilio calvanese strinati",
            "walid saad"
        ]
    },
    {
        "id": "2311.18232",
        "title": "lmrl gym: benchmarks for multi-turn reinforcement learning with language   models",
        "abstract": "large language models (llms) provide excellent text-generation capabilities, but standard prompting and generation methods generally do not lead to intentional or goal-directed agents and might necessitate considerable prompt tuning. this becomes particularly apparent in multi-turn conversations: even the best current llms rarely ask clarifying questions, engage in explicit information gathering, or take actions now that lead to better decisions after multiple turns. reinforcement learning has the potential to leverage the powerful modeling capabilities of llms, as well as their internal representation of textual interactions, to create capable goal-directed language agents. this can enable intentional and temporally extended interactions, such as with humans, through coordinated persuasion and carefully crafted questions, or in goal-directed play through text games to bring about desired final outcomes. however, enabling this requires the community to develop stable and reliable reinforcement learning algorithms that can effectively train llms. developing such algorithms requires tasks that can gauge progress on algorithm design, provide accessible and reproducible evaluations for multi-turn interactions, and cover a range of task properties and challenges in improving reinforcement learning algorithms. our paper introduces the lmrl-gym benchmark for evaluating multi-turn rl for llms, together with an open-source research framework containing a basic toolkit for getting started on multi-turn rl with offline value-based and policy-based rl methods. our benchmark consists of 8 different language tasks, which require multiple rounds of language interaction and cover a range of tasks in open-ended dialogue and text games.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.18232",
        "authors": [
            "marwa abdulhai",
            "isadora white",
            "charlie snell",
            "charles sun",
            "joey hong",
            "yuexiang zhai",
            "kelvin xu",
            "sergey levine"
        ]
    },
    {
        "id": "2311.18241",
        "title": "llvms4protest: harnessing the power of large language and vision models   for deciphering protests in the news",
        "abstract": "large language and vision models have transformed how social movements scholars identify protest and extract key protest attributes from multi-modal data such as texts, images, and videos. this article documents how we fine-tuned two large pretrained transformer models, including longformer and swin-transformer v2, to infer potential protests in news articles using textual and imagery data. first, the longformer model was fine-tuned using the dynamic of collective action (doca) corpus. we matched the new york times articles with the doca database to obtain a training dataset for downstream tasks. second, the swin-transformer v2 models was trained on ucla-protest imagery data. ucla-protest project contains labeled imagery data with information such as protest, violence, and sign. both fine-tuned models will be available via \\url{https://github.com/joshzyj/llvms4protest}. we release this short technical report for social movement scholars who are interested in using llvms to infer protests in textual and imagery data.",
        "doi": "",
        "created": "2023-11-29",
        "url": "https://arxiv.org/abs/2311.18241",
        "authors": [
            "yongjun zhang"
        ]
    },
    {
        "id": "2311.18252",
        "title": "navigating privacy and copyright challenges across the data lifecycle of   generative ai",
        "abstract": "the advent of generative ai has marked a significant milestone in artificial intelligence, demonstrating remarkable capabilities in generating realistic images, texts, and data patterns. however, these advancements come with heightened concerns over data privacy and copyright infringement, primarily due to the reliance on vast datasets for model training. traditional approaches like differential privacy, machine unlearning, and data poisoning only offer fragmented solutions to these complex issues. our paper delves into the multifaceted challenges of privacy and copyright protection within the data lifecycle. we advocate for integrated approaches that combines technical innovation with ethical foresight, holistically addressing these concerns by investigating and devising solutions that are informed by the lifecycle perspective. this work aims to catalyze a broader discussion and inspire concerted efforts towards data privacy and copyright integrity in generative ai.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18252",
        "authors": [
            "dawen zhang",
            "boming xia",
            "yue liu",
            "xiwei xu",
            "thong hoang",
            "zhenchang xing",
            "mark staples",
            "qinghua lu",
            "liming zhu"
        ]
    },
    {
        "id": "2311.18254",
        "title": "sketch input method editor: a comprehensive dataset and methodology for   systematic input recognition",
        "abstract": "with the recent surge in the use of touchscreen devices, free-hand sketching has emerged as a promising modality for human-computer interaction. while previous research has focused on tasks such as recognition, retrieval, and generation of familiar everyday objects, this study aims to create a sketch input method editor (sketchime) specifically designed for a professional c4i system. within this system, sketches are utilized as low-fidelity prototypes for recommending standardized symbols in the creation of comprehensive situation maps. this paper also presents a systematic dataset comprising 374 specialized sketch types, and proposes a simultaneous recognition and segmentation architecture with multilevel supervision between recognition and segmentation to improve performance and enhance interpretability. by incorporating few-shot domain adaptation and class-incremental learning, the network's ability to adapt to new users and extend to new task-specific classes is significantly enhanced. results from experiments conducted on both the proposed dataset and the spg dataset illustrate the superior performance of the proposed architecture. our dataset and code are publicly available at https://github.com/anony517/sketchime.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18254",
        "authors": [
            "guangming zhu",
            "siyuan wang",
            "qing cheng",
            "kelong wu",
            "hao li",
            "liang zhang"
        ]
    },
    {
        "id": "2311.18259",
        "title": "ego-exo4d: understanding skilled human activity from first- and   third-person perspectives",
        "abstract": "we present ego-exo4d, a diverse, large-scale multimodal multiview video dataset and benchmark challenge. ego-exo4d centers around simultaneously-captured egocentric and exocentric video of skilled human activities (e.g., sports, music, dance, bike repair). more than 800 participants from 13 cities worldwide performed these activities in 131 different natural scene contexts, yielding long-form captures from 1 to 42 minutes each and 1,422 hours of video combined. the multimodal nature of the dataset is unprecedented: the video is accompanied by multichannel audio, eye gaze, 3d point clouds, camera poses, imu, and multiple paired language descriptions -- including a novel \"expert commentary\" done by coaches and teachers and tailored to the skilled-activity domain. to push the frontier of first-person video understanding of skilled human activity, we also present a suite of benchmark tasks and their annotations, including fine-grained activity understanding, proficiency estimation, cross-view translation, and 3d hand/body pose. all resources will be open sourced to fuel new research in the community.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18259",
        "authors": [
            "kristen grauman",
            "andrew westbury",
            "lorenzo torresani",
            "kris kitani",
            "jitendra malik",
            "triantafyllos afouras",
            "kumar ashutosh",
            "vijay baiyya",
            "siddhant bansal",
            "bikram boote",
            "eugene byrne",
            "zach chavis",
            "joya chen",
            "feng cheng",
            "fu-jen chu",
            "sean crane",
            "avijit dasgupta",
            "jing dong",
            "maria escobar",
            "cristhian forigua",
            "abrham gebreselasie",
            "sanjay haresh",
            "jing huang",
            "md mohaiminul islam",
            "suyog jain",
            "rawal khirodkar",
            "devansh kukreja",
            "kevin j liang",
            "jia-wei liu",
            "sagnik majumder",
            "yongsen mao",
            "miguel martin",
            "effrosyni mavroudi",
            "tushar nagarajan",
            "francesco ragusa",
            "santhosh kumar ramakrishnan",
            "luigi seminara",
            "arjun somayazulu",
            "yale song",
            "shan su",
            "zihui xue",
            "edward zhang",
            "jinxu zhang",
            "angela castillo",
            "changan chen",
            "xinzhu fu",
            "ryosuke furuta",
            "cristina gonzalez",
            "prince gupta",
            "jiabo hu",
            "yifei huang",
            "yiming huang",
            "weslie khoo",
            "anush kumar",
            "robert kuo",
            "sach lakhavani",
            "miao liu",
            "mi luo",
            "zhengyi luo",
            "brighid meredith",
            "austin miller",
            "oluwatumininu oguntola",
            "xiaqing pan",
            "penny peng",
            "shraman pramanick",
            "merey ramazanova",
            "fiona ryan",
            "wei shan",
            "kiran somasundaram",
            "chenan song",
            "audrey southerland",
            "masatoshi tateno",
            "huiyu wang",
            "yuchen wang",
            "takuma yagi",
            "mingfei yan",
            "xitong yang",
            "zecheng yu",
            "shengxin cindy zha",
            "chen zhao",
            "ziwei zhao",
            "zhifan zhu",
            "jeff zhuo",
            "pablo arbelaez",
            "gedas bertasius",
            "david crandall",
            "dima damen",
            "jakob engel",
            "giovanni maria farinella",
            "antonino furnari",
            "bernard ghanem",
            "judy hoffman",
            "c. v. jawahar",
            "richard newcombe",
            "hyun soo park",
            "james m. rehg",
            "yoichi sato",
            "manolis savva",
            "jianbo shi",
            "mike zheng shou",
            "michael wray"
        ]
    },
    {
        "id": "2311.18296",
        "title": "perceptual group tokenizer: building perception with iterative grouping",
        "abstract": "human visual recognition system shows astonishing capability of compressing visual information into a set of tokens containing rich representations without label supervision. one critical driving principle behind it is perceptual grouping. despite being widely used in computer vision in the early 2010s, it remains a mystery whether perceptual grouping can be leveraged to derive a neural visual recognition backbone that generates as powerful representations. in this paper, we propose the perceptual group tokenizer, a model that entirely relies on grouping operations to extract visual features and perform self-supervised representation learning, where a series of grouping operations are used to iteratively hypothesize the context for pixels or superpixels to refine feature representations. we show that the proposed model can achieve competitive performance compared to state-of-the-art vision architectures, and inherits desirable properties including adaptive computation without re-training, and interpretability. specifically, perceptual group tokenizer achieves 80.3% on imagenet-1k self-supervised learning benchmark with linear probe evaluation, marking a new progress under this paradigm.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18296",
        "authors": [
            "zhiwei deng",
            "ting chen",
            "yang li"
        ]
    },
    {
        "id": "2311.18297",
        "title": "trustmark: universal watermarking for arbitrary resolution images",
        "abstract": "imperceptible digital watermarking is important in copyright protection, misinformation prevention, and responsible generative ai. we propose trustmark - a gan-based watermarking method with novel design in architecture and spatio-spectra losses to balance the trade-off between watermarked image quality with the watermark recovery accuracy. our model is trained with robustness in mind, withstanding various in- and out-place perturbations on the encoded image. additionally, we introduce trustmark-rm - a watermark remover method useful for re-watermarking. our methods achieve state-of-art performance on 3 benchmarks comprising arbitrary resolution images.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18297",
        "authors": [
            "tu bui",
            "shruti agarwal",
            "john collomosse"
        ]
    },
    {
        "id": "2311.18328",
        "title": "advances in 3d neural stylization: a survey",
        "abstract": "modern artificial intelligence provides a novel way of producing digital art in styles. the expressive power of neural networks enables the realm of visual style transfer methods, which can be used to edit images, videos, and 3d data to make them more artistic and diverse. this paper reports on recent advances in neural stylization for 3d data. we provide a taxonomy for neural stylization by considering several important design choices, including scene representation, guidance data, optimization strategies, and output styles. building on such taxonomy, our survey first revisits the background of neural stylization on 2d images, and then provides in-depth discussions on recent neural stylization methods for 3d data, where we also provide a mini-benchmark on artistic stylization methods. based on the insights gained from the survey, we then discuss open challenges, future research, and potential applications and impacts of neural stylization.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18328",
        "authors": [
            "yingshu chen",
            "guocheng shao",
            "ka chun shum",
            "binh-son hua",
            "sai-kit yeung"
        ]
    },
    {
        "id": "2311.18331",
        "title": "mrfp: learning generalizable semantic segmentation from sim-2-real with   multi-resolution feature perturbation",
        "abstract": "deep neural networks have shown exemplary performance on semantic scene understanding tasks on source domains, but due to the absence of style diversity during training, enhancing performance on unseen target domains using only single source domain data remains a challenging task. generation of simulated data is a feasible alternative to retrieving large style-diverse real-world datasets as it is a cumbersome and budget-intensive process. however, the large domain-specific inconsistencies between simulated and real-world data pose a significant generalization challenge in semantic segmentation. in this work, to alleviate this problem, we propose a novel multiresolution feature perturbation (mrfp) technique to randomize domain-specific fine-grained features and perturb style of coarse features. our experimental results on various urban-scene segmentation datasets clearly indicate that, along with the perturbation of style-information, perturbation of fine-feature components is paramount to learn domain invariant robust feature maps for semantic segmentation models. mrfp is a simple and computationally efficient, transferable module with no additional learnable parameters or objective functions, that helps state-of-the-art deep neural networks to learn robust domain invariant features for simulation-to-real semantic segmentation.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18331",
        "authors": [
            "sumanth udupa",
            "prajwal gurunath",
            "aniruddh sikdar",
            "suresh sundaram"
        ]
    },
    {
        "id": "2311.18403",
        "title": "corrupting convolution-based unlearnable datasets with pixel-based image   transformations",
        "abstract": "unlearnable datasets lead to a drastic drop in the generalization performance of models trained on them by introducing elaborate and imperceptible perturbations into clean training sets. many existing defenses, e.g., jpeg compression and adversarial training, effectively counter uds based on norm-constrained additive noise. however, a fire-new type of convolution-based uds have been proposed and render existing defenses all ineffective, presenting a greater challenge to defenders. to address this, we express the convolution-based unlearnable sample as the result of multiplying a matrix by a clean sample in a simplified scenario, and formalize the intra-class matrix inconsistency as $\\theta_{imi}$, inter-class matrix consistency as $\\theta_{imc}$ to investigate the working mechanism of the convolution-based uds. we conjecture that increasing both of these metrics will mitigate the unlearnability effect. through validation experiments that commendably support our hypothesis, we further design a random matrix to boost both $\\theta_{imi}$ and $\\theta_{imc}$, achieving a notable degree of defense effect. hence, by building upon and extending these facts, we first propose a brand-new image corruption that employs randomly multiplicative transformation via interpolation operation to successfully defend against convolution-based uds. our approach leverages global pixel random interpolations, effectively suppressing the impact of multiplicative noise in convolution-based uds. additionally, we have also designed two new forms of convolution-based uds, and find that our defense is the most effective against them.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18403",
        "authors": [
            "xianlong wang",
            "shengshan hu",
            "minghui li",
            "zhifei yu",
            "ziqi zhou",
            "leo yu zhang",
            "hai jin"
        ]
    },
    {
        "id": "2311.18424",
        "title": "multiple disciplinary data work practices in artificial intelligence   research: a healthcare case study in the uk",
        "abstract": "developing artificial intelligence (ai) tools for healthcare is a multiple disciplinary effort, bringing data scientists, clinicians, patients and other disciplines together. in this paper, we explore the ai development workflow and how participants navigate the challenges and tensions of sharing and generating knowledge across disciplines. through an inductive thematic analysis of 13 semi-structured interviews with participants in a large research consortia, our findings suggest that multiple disciplinarity heavily impacts work practices. participants faced challenges to learn the languages of other disciplines and needed to adapt the tools used for sharing and communicating with their audience, particularly those from a clinical or patient perspective. large health datasets also posed certain restrictions on work practices. we identified meetings as a key platform for facilitating exchanges between disciplines and allowing for the blending and creation of knowledge. finally, we discuss design implications for data science and collaborative tools, and recommendations for future research.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18424",
        "authors": [
            "rafael henkin",
            "elizabeth remfry",
            "duncan j. reynolds",
            "megan clinch",
            "michael r. barnes"
        ]
    },
    {
        "id": "2311.18460",
        "title": "causal fairness under unobserved confounding: a neural sensitivity   framework",
        "abstract": "fairness for machine learning predictions is widely required in practice for legal, ethical, and societal reasons. existing work typically focuses on settings without unobserved confounding, even though unobserved confounding can lead to severe violations of causal fairness and, thus, unfair predictions. in this work, we analyze the sensitivity of causal fairness to unobserved confounding. our contributions are three-fold. first, we derive bounds for causal fairness metrics under different sources of unobserved confounding. this enables practitioners to examine the sensitivity of their machine learning models to unobserved confounding in fairness-critical applications. second, we propose a novel neural framework for learning fair predictions, which allows us to offer worst-case guarantees of the extent to which causal fairness can be violated due to unobserved confounding. third, we demonstrate the effectiveness of our framework in a series of experiments, including a real-world case study about predicting prison sentences. to the best of our knowledge, ours is the first work to study causal fairness under unobserved confounding. to this end, our work is of direct practical value as a refutation strategy to ensure the fairness of predictions in high-stakes applications.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18460",
        "authors": [
            "maresa schr\u00f6der",
            "dennis frauen",
            "stefan feuerriegel"
        ]
    },
    {
        "id": "2311.18481",
        "title": "esg accountability made easy: docqa at your service",
        "abstract": "we present deep search docqa. this application enables information extraction from documents via a question-answering conversational assistant. the system integrates several technologies from different ai disciplines consisting of document conversion to machine-readable format (via computer vision), finding relevant data (via natural language processing), and formulating an eloquent response (via large language models). users can explore over 10,000 environmental, social, and governance (esg) disclosure reports from over 2000 corporations. the deep search platform can be accessed at: https://ds4sd.github.io.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18481",
        "authors": [
            "lokesh mishra",
            "cesar berrospi",
            "kasper dinkla",
            "diego antognini",
            "francesco fusco",
            "benedikt bothur",
            "maksym lysak",
            "nikolaos livathinos",
            "ahmed nassar",
            "panagiotis vagenas",
            "lucas morin",
            "christoph auer",
            "michele dolfi",
            "peter staar"
        ]
    },
    {
        "id": "2311.18486",
        "title": "new perspectives on the evaluation of link prediction algorithms for   dynamic graphs",
        "abstract": "there is a fast-growing body of research on predicting future links in dynamic networks, with many new algorithms. some benchmark data exists, and performance evaluations commonly rely on comparing the scores of observed network events (positives) with those of randomly generated ones (negatives). these evaluation measures depend on both the predictive ability of the model and, crucially, the type of negative samples used. besides, as generally the case with temporal data, prediction quality may vary over time. this creates a complex evaluation space. in this work, we catalog the possibilities for negative sampling and introduce novel visualization methods that can yield insight into prediction performance and the dynamics of temporal networks. we leverage these visualization tools to investigate the effect of negative sampling on the predictive performance, at the node and edge level. we validate empirically, on datasets extracted from recent benchmarks that the error is typically not evenly distributed across different data segments. finally, we argue that such visualization tools can serve as powerful guides to evaluate dynamic link prediction methods at different levels.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18486",
        "authors": [
            "rapha\u00ebl romero",
            "tijl de bie",
            "jefrey lijffijt"
        ]
    },
    {
        "id": "2311.18491",
        "title": "zest-nerf: using temporal aggregation for zero-shot temporal nerfs",
        "abstract": "in the field of media production, video editing techniques play a pivotal role. recent approaches have had great success at performing novel view image synthesis of static scenes. but adding temporal information adds an extra layer of complexity. previous models have focused on implicitly representing static and dynamic scenes using nerf. these models achieve impressive results but are costly at training and inference time. they overfit an mlp to describe the scene implicitly as a function of position. this paper proposes zest-nerf, a new approach that can produce temporal nerfs for new scenes without retraining. we can accurately reconstruct novel views using multi-view synthesis techniques and scene flow-field estimation, trained only with unrelated scenes. we demonstrate how existing state-of-the-art approaches from a range of fields cannot adequately solve this new task and demonstrate the efficacy of our solution. the resulting network improves quantitatively by 15% and produces significantly better visual results.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18491",
        "authors": [
            "violeta men\u00e9ndez gonz\u00e1lez",
            "andrew gilbert",
            "graeme phillipson",
            "stephen jolly",
            "simon hadfield"
        ]
    },
    {
        "id": "2311.18518",
        "title": "color-emotion associations in art: fuzzy approach",
        "abstract": "art objects can evoke certain emotions. color is a fundamental element of visual art and plays a significant role in how art is perceived. this paper introduces a novel approach to classifying emotions in art using fuzzy sets. we employ a fuzzy approach because it aligns well with human judgments' imprecise and subjective nature. extensive fuzzy colors (n=120) and a broad emotional spectrum (n=10) allow for a more human-consistent and context-aware exploration of emotions inherent in paintings. first, we introduce the fuzzy color representation model. then, at the fuzzification stage, we process the wiki art dataset of paintings tagged with emotions, extracting fuzzy dominant colors linked to specific emotions. this results in fuzzy color distributions for ten emotions. finally, we convert them back to a crisp domain, obtaining a knowledge base of color-emotion associations in primary colors. our findings reveal strong associations between specific emotions and colors; for instance, gratitude strongly correlates with green, brown, and orange. other noteworthy associations include brown and anger, orange with shame, yellow with happiness, and gray with fear. using these associations and jaccard similarity, we can find the emotions in the arbitrary untagged image. we conducted a 2afc experiment involving human subjects to evaluate the proposed method. the average hit rate of 0.77 indicates a significant correlation between the method's predictions and human perception. the proposed method is simple to adapt to art painting retrieval systems. the study contributes to the theoretical understanding of color-emotion associations in art, offering valuable insights for various practical applications besides art, like marketing, design, and psychology.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18518",
        "authors": [
            "pakizar shamoi",
            "muragul muratbekova"
        ]
    },
    {
        "id": "2311.18520",
        "title": "calibration-free online test-time adaptation for electroencephalography   motor imagery decoding",
        "abstract": "providing a promising pathway to link the human brain with external devices, brain-computer interfaces (bcis) have seen notable advancements in decoding capabilities, primarily driven by increasingly sophisticated techniques, especially deep learning. however, achieving high accuracy in real-world scenarios remains a challenge due to the distribution shift between sessions and subjects. in this paper we will explore the concept of online test-time adaptation (otta) to continuously adapt the model in an unsupervised fashion during inference time. our approach guarantees the preservation of privacy by eliminating the requirement to access the source data during the adaptation process. additionally, otta achieves calibration-free operation by not requiring any session- or subject-specific data. we will investigate the task of electroencephalography (eeg) motor imagery decoding using a lightweight architecture together with different otta techniques like alignment, adaptive batch normalization, and entropy minimization. we examine two datasets and three distinct data settings for a comprehensive analysis. our adaptation methods produce state-of-the-art results, potentially instigating a shift in transfer learning for bci decoding towards online adaptation.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18520",
        "authors": [
            "martin wimpff",
            "mario d\u00f6bler",
            "bin yang"
        ]
    },
    {
        "id": "2311.18531",
        "title": "dataset distillation via the wasserstein metric",
        "abstract": "dataset distillation (dd) offers a compelling approach in computer vision, with the goal of condensing extensive datasets into smaller synthetic versions without sacrificing much of the model performance. in this paper, we continue to study the methods for dd, by addressing its conceptually core objective: how to capture the essential representation of extensive datasets in smaller, synthetic forms.   we propose a novel approach utilizing the wasserstein distance, a metric rooted in optimal transport theory, to enhance distribution matching in dd. our method leverages the wasserstein barycenter, offering a geometrically meaningful way to quantify distribution differences and effectively capture the centroid of a set of distributions. our approach retains the computational benefits of distribution matching-based methods while achieving new state-of-the-art performance on several benchmarks.   to provide useful prior for learning the images, we embed the synthetic data into the feature space of pretrained classification models to conduct distribution matching. extensive testing on various high-resolution datasets confirms the effectiveness and adaptability of our method, indicating the promising yet unexplored capabilities of wasserstein metrics in dataset distillation.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18531",
        "authors": [
            "haoyang liu",
            "tiancheng xing",
            "luwei li",
            "vibhu dalal",
            "jingrui he",
            "haohan wang"
        ]
    },
    {
        "id": "2311.18547",
        "title": "real-time vibration-based bearing fault diagnosis under time-varying   speed conditions",
        "abstract": "detection of rolling-element bearing faults is crucial for implementing proactive maintenance strategies and for minimizing the economic and operational consequences of unexpected failures. however, many existing techniques are developed and tested under strictly controlled conditions, limiting their adaptability to the diverse and dynamic settings encountered in practical applications. this paper presents an efficient real-time convolutional neural network (cnn) for diagnosing multiple bearing faults under various noise levels and time-varying rotational speeds. additionally, we propose a novel fisher-based spectral separability analysis (ssa) method to elucidate the effectiveness of the designed cnn model. we conducted experiments on both healthy bearings and bearings afflicted with inner race, outer race, and roller ball faults. the experimental results show the superiority of our model over the current state-of-the-art approach in three folds: it achieves substantial accuracy gains of up to 15.8%, it is robust to noise with high performance across various signal-to-noise ratios, and it runs in real-time with processing durations five times less than acquisition. additionally, by using the proposed ssa technique, we offer insights into the model's performance and underscore its effectiveness in tackling real-world challenges.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18547",
        "authors": [
            "tuomas jalonen",
            "mohammad al-sa'd",
            "serkan kiranyaz",
            "moncef gabbouj"
        ]
    },
    {
        "id": "2311.18550",
        "title": "search still matters: information retrieval in the era of generative ai",
        "abstract": "objective: information retrieval (ir, also known as search) systems are ubiquitous in modern times. how does the emergence of generative artificial intelligence (ai), based on large language models (llms), fit into the ir process? process: this perspective explores the use of generative ai in the context of the motivations, considerations, and outcomes of the ir process with a focus on the academic use of such systems. conclusions: there are many information needs, from simple to complex, that motivate use of ir. users of such systems, particularly academics, have concerns for authoritativeness, timeliness, and contextualization of search. while llms may provide functionality that aids the ir process, the continued need for search systems, and research into their improvement, remains essential.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18550",
        "authors": [
            "william r. hersh"
        ]
    },
    {
        "id": "2311.18576",
        "title": "fingerprint matching with localized deep representation",
        "abstract": "compared to minutia-based fingerprint representations, fixed-length representations are attractive due to simple and efficient matching. however, fixed-length fingerprint representations are limited in accuracy when matching fingerprints with different visible areas, which can occur due to different finger poses or acquisition methods. to address this issue, we propose a localized deep representation of fingerprint, named ldrf. by focusing on the discriminative characteristics within local regions, ldrf provides a more robust and accurate fixed-length representation for fingerprints with variable visible areas. ldrf can be adapted to retain information within any valid area, making it highly flexible. the matching scores produced by ldrf also exhibit intuitive statistical characteristics, which led us to propose a matching score normalization technique to mitigate the uncertainty in the cases of very small overlapping area. with this new technique, we can maintain a high level of accuracy and reliability in our fingerprint matching, even as the size of the database grows rapidly. our experimental results on 21 datasets containing over 140k fingerprints of various finger poses and impression types show that ldrf outperforms other fixed-length representations and is robust to sensing technologies and impression types. besides, the proposed matching score normalization effectively reduces the false match rate (fmr) in large-scale identification experiments comprising over 5.11 million fingerprints. specifically, this technique results in a reduction of two orders of magnitude compared to matching without matching score normalization and five orders of magnitude compared to prior works.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18576",
        "authors": [
            "yongjie duan",
            "zhiyu pan",
            "jianjiang feng",
            "jie zhou"
        ]
    },
    {
        "id": "2311.18578",
        "title": "communication-efficient heterogeneous federated learning with   generalized heavy-ball momentum",
        "abstract": "federated learning (fl) is the state-of-the-art approach for learning from decentralized data in privacy-constrained scenarios. as the current literature reports, the main problems associated with fl refer to system and statistical challenges: the former ones demand for efficient learning from edge devices, including lowering communication bandwidth and frequency, while the latter require algorithms robust to non-iidness. state-of-art approaches either guarantee convergence at increased communication cost or are not sufficiently robust to handle extreme heterogeneous local distributions. in this work we propose a novel generalization of the heavy-ball momentum, and present fedhbm to effectively address statistical heterogeneity in fl without introducing any communication overhead. we conduct extensive experimentation on common fl vision and nlp datasets, showing that our fedhbm algorithm empirically yields better model quality and higher convergence speed w.r.t. the state-of-art, especially in pathological non-iid scenarios. while being designed for cross-silo settings, we show how fedhbm is applicable in moderate-to-high cross-device scenarios, and how good model initializations (e.g. pre-training) can be exploited for prompt acceleration. extended experimentation on large-scale real-world federated datasets further corroborates the effectiveness of our approach for real-world fl applications.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18578",
        "authors": [
            "riccardo zaccone",
            "carlo masone",
            "marco ciccone"
        ]
    },
    {
        "id": "2311.18587",
        "title": "continuous 16-bit training: accelerating 32-bit pre-trained neural   networks",
        "abstract": "in the field of deep learning, the prevalence of models initially trained with 32-bit precision is a testament to its robustness and accuracy. however, the continuous evolution of these models often demands further training, which can be resource-intensive. this study introduces a novel approach where we continue the training of these pre-existing 32-bit models using 16-bit precision. this technique not only caters to the need for efficiency in computational resources but also significantly improves the speed of additional training phases. by adopting 16-bit precision for ongoing training, we are able to substantially decrease memory requirements and computational burden, thereby accelerating the training process in a resource-limited setting. our experiments show that this method maintains the high standards of accuracy set by the original 32-bit training while providing a much-needed boost in training speed. this approach is especially pertinent in today's context, where most models are initially trained in 32-bit and require periodic updates and refinements. the findings from our research suggest that this strategy of 16-bit continuation training can be a key solution for sustainable and efficient deep learning, offering a practical way to enhance pre-trained models rapidly and in a resource-conscious manner.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18587",
        "authors": [
            "juyoung yun"
        ]
    },
    {
        "id": "2311.18592",
        "title": "semantic-aware frame-event fusion based pattern recognition via large   vision-language models",
        "abstract": "pattern recognition through the fusion of rgb frames and event streams has emerged as a novel research area in recent years. current methods typically employ backbone networks to individually extract the features of rgb frames and event streams, and subsequently fuse these features for pattern recognition. however, we posit that these methods may suffer from key issues like sematic gaps and small-scale backbone networks. in this study, we introduce a novel pattern recognition framework that consolidates the semantic labels, rgb frames, and event streams, leveraging pre-trained large-scale vision-language models. specifically, given the input rgb frames, event streams, and all the predefined semantic labels, we employ a pre-trained large-scale vision model (clip vision encoder) to extract the rgb and event features. to handle the semantic labels, we initially convert them into language descriptions through prompt engineering, and then obtain the semantic features using the pre-trained large-scale language model (clip text encoder). subsequently, we integrate the rgb/event features and semantic features using multimodal transformer networks. the resulting frame and event tokens are further amplified using self-attention layers. concurrently, we propose to enhance the interactions between text tokens and rgb/event tokens via cross-attention. finally, we consolidate all three modalities using self-attention and feed-forward layers for recognition. comprehensive experiments on the hardvs and pokerevent datasets fully substantiate the efficacy of our proposed safe model. the source code will be made available at https://github.com/event-ahu/safe_largevlm.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18592",
        "authors": [
            "dong li",
            "jiandong jin",
            "yuhao zhang",
            "yanlin zhong",
            "yaoyang wu",
            "lan chen",
            "xiao wang",
            "bin luo"
        ]
    },
    {
        "id": "2311.18598",
        "title": "generalisable agents for neural network optimisation",
        "abstract": "optimising deep neural networks is a challenging task due to complex training dynamics, high computational requirements, and long training times. to address this difficulty, we propose the framework of generalisable agents for neural network optimisation (ganno) -- a multi-agent reinforcement learning (marl) approach that learns to improve neural network optimisation by dynamically and responsively scheduling hyperparameters during training. ganno utilises an agent per layer that observes localised network dynamics and accordingly takes actions to adjust these dynamics at a layerwise level to collectively improve global performance. in this paper, we use ganno to control the layerwise learning rate and show that the framework can yield useful and responsive schedules that are competitive with handcrafted heuristics. furthermore, ganno is shown to perform robustly across a wide variety of unseen initial conditions, and can successfully generalise to harder problems than it was trained on. our work presents an overview of the opportunities that this paradigm offers for training neural networks, along with key challenges that remain to be overcome.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18598",
        "authors": [
            "kale-ab tessera",
            "callum rhys tilbury",
            "sasha abramowitz",
            "ruan de kock",
            "omayma mahjoub",
            "benjamin rosman",
            "sara hooker",
            "arnu pretorius"
        ]
    },
    {
        "id": "2311.18599",
        "title": "joint detection algorithm for multiple cognitive users in spectrum   sensing",
        "abstract": "spectrum sensing technology is a crucial aspect of modern communication technology, serving as one of the essential techniques for efficiently utilizing scarce information resources in tight frequency bands. this paper first introduces three common logical circuit decision criteria in hard decisions and analyzes their decision rigor. building upon hard decisions, the paper further introduces a method for multi-user spectrum sensing based on soft decisions. then the paper simulates the false alarm probability and detection probability curves corresponding to the three criteria. the simulated results of multi-user collaborative sensing indicate that the simulation process significantly reduces false alarm probability and enhances detection probability. this approach effectively detects spectrum resources unoccupied during idle periods, leveraging the concept of time-division multiplexing and rationalizing the redistribution of information resources. the entire computation process relies on the calculation principles of power spectral density in communication theory, involving threshold decision detection for noise power and the sum of noise and signal power. it provides a secondary decision detection, reflecting the perceptual decision performance of logical detection methods with relative accuracy.",
        "doi": "10.54254/2977-3903/4/2023053.",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18599",
        "authors": [
            "fanfei meng",
            "yuxin wang",
            "lele zhang",
            "yingxin zhao",
            "david demeter"
        ]
    },
    {
        "id": "2311.18608",
        "title": "contrastive denoising score for text-guided latent diffusion image   editing",
        "abstract": "with the remarkable advent of text-to-image diffusion models, image editing methods have become more diverse and continue to evolve. a promising recent approach in this realm is delta denoising score (dds) - an image editing technique based on score distillation sampling (sds) framework that leverages the rich generative prior of text-to-image diffusion models. however, relying solely on the difference between scoring functions is insufficient for preserving specific structural elements from the original image, a crucial aspect of image editing. inspired by the similarity and importance differences between dds and the contrastive learning for unpaired image-to-image translation (cut), here we present an embarrassingly simple yet very powerful modification of dds, called contrastive denoising score (cds), for latent diffusion models (ldm). specifically, to enforce structural correspondence between the input and output while maintaining the controllability of contents, we introduce a straightforward approach to regulate structural consistency using cut loss within the dds framework. to calculate this loss, instead of employing auxiliary networks, we utilize the intermediate features of ldm, in particular, those from the self-attention layers, which possesses rich spatial information. our approach enables zero-shot image-to-image translation and neural radiance field (nerf) editing, achieving a well-balanced interplay between maintaining the structural details and transforming content. qualitative results and comparisons demonstrates the effectiveness of our proposed method. project page with code is available at https://hyelinnam.github.io/cds/.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18608",
        "authors": [
            "hyelin nam",
            "gihyun kwon",
            "geon yeong park",
            "jong chul ye"
        ]
    },
    {
        "id": "2311.18620",
        "title": "data-driven prediction of tool wear using bayesian-regularized   artificial neural networks",
        "abstract": "the prediction of tool wear helps minimize costs and enhance product quality in manufacturing. while existing data-driven models using machine learning and deep learning have contributed to the accurate prediction of tool wear, they often lack generality and require substantial training data for high accuracy. in this paper, we propose a new data-driven model that uses bayesian regularized artificial neural networks (branns) to precisely predict milling tool wear. branns combine the strengths and leverage the benefits of artificial neural networks (anns) and bayesian regularization, whereby anns learn complex patterns and bayesian regularization handles uncertainty and prevents overfitting, resulting in a more generalized model. we treat both process parameters and monitoring sensor signals as brann input parameters. we conducted an extensive experimental study featuring four different experimental data sets, including the nasa ames milling dataset, the 2010 phm data challenge dataset, the nuaa ideahouse tool wear dataset, and an in-house performed end-milling of the ti6al4v dataset. we inspect the impact of input features, training data size, hidden units, training algorithms, and transfer functions on the performance of the proposed brann model and demonstrate that it outperforms existing state-of-the-art models in terms of accuracy and reliability.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18620",
        "authors": [
            "tam t. truong",
            "jay airao",
            "panagiotis karras",
            "faramarz hojati",
            "bahman azarhoushang",
            "ramin aghababaei"
        ]
    },
    {
        "id": "2311.18636",
        "title": "end-to-end autonomous driving using deep learning: a systematic review",
        "abstract": "end-to-end autonomous driving is a fully differentiable machine learning system that takes raw sensor input data and other metadata as prior information and directly outputs the ego vehicle's control signals or planned trajectories. this paper attempts to systematically review all recent machine learning-based techniques to perform this end-to-end task, including, but not limited to, object detection, semantic scene understanding, object tracking, trajectory predictions, trajectory planning, vehicle control, social behavior, and communications. this paper focuses on recent fully differentiable end-to-end reinforcement learning and deep learning-based techniques. our paper also builds taxonomies of the significant approaches by sub-grouping them and showcasing their research trends. finally, this survey highlights the open challenges and points out possible future directions to enlighten further research on the topic.",
        "doi": "",
        "created": "2023-08-27",
        "url": "https://arxiv.org/abs/2311.18636",
        "authors": [
            "apoorv singh"
        ]
    },
    {
        "id": "2311.18644",
        "title": "exploring the hierarchical structure of human plans via program   generation",
        "abstract": "human behavior is inherently hierarchical, resulting from the decomposition of a task into subtasks or an abstract action into concrete actions. however, behavior is typically measured as a sequence of actions, which makes it difficult to infer its hierarchical structure. in this paper, we explore how people form hierarchically-structured plans, using an experimental paradigm that makes hierarchical representations observable: participants create programs that produce sequences of actions in a language with explicit hierarchical structure. this task lets us test two well-established principles of human behavior: utility maximization (i.e. using fewer actions) and minimum description length (mdl; i.e. having a shorter program). we find that humans are sensitive to both metrics, but that both accounts fail to predict a qualitative feature of human-created programs, namely that people prefer programs with reuse over and above the predictions of mdl. we formalize this preference for reuse by extending the mdl account into a generative model over programs, modeling hierarchy choice as the induction of a grammar over actions. our account can explain the preference for reuse and provides the best prediction of human behavior, going beyond simple accounts of compressibility to highlight a principle that guides hierarchical planning.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18644",
        "authors": [
            "carlos g. correa",
            "sophia sanborn",
            "mark k. ho",
            "frederick callaway",
            "nathaniel d. daw",
            "thomas l. griffiths"
        ]
    },
    {
        "id": "2311.18645",
        "title": "stochastic vision transformers with wasserstein distance-aware attention",
        "abstract": "self-supervised learning is one of the most promising approaches to acquiring knowledge from limited labeled data. despite the substantial advancements made in recent years, self-supervised models have posed a challenge to practitioners, as they do not readily provide insight into the model's confidence and uncertainty. tackling this issue is no simple feat, primarily due to the complexity involved in implementing techniques that can make use of the latent representations learned during pre-training without relying on explicit labels. motivated by this, we introduce a new stochastic vision transformer that integrates uncertainty and distance awareness into self-supervised learning (ssl) pipelines. instead of the conventional deterministic vector embedding, our novel stochastic vision transformer encodes image patches into elliptical gaussian distributional embeddings. notably, the attention matrices of these stochastic representational embeddings are computed using wasserstein distance-based attention, effectively capitalizing on the distributional nature of these embeddings. additionally, we propose a regularization term based on wasserstein distance for both pre-training and fine-tuning processes, thereby incorporating distance awareness into latent representations. we perform extensive experiments across different tasks such as in-distribution generalization, out-of-distribution detection, dataset corruption, semi-supervised settings, and transfer learning to other datasets and tasks. our proposed method achieves superior accuracy and calibration, surpassing the self-supervised baseline in a wide range of experiments on a variety of datasets.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18645",
        "authors": [
            "franciskus xaverius erick",
            "mina rezaei",
            "johanna paula m\u00fcller",
            "bernhard kainz"
        ]
    },
    {
        "id": "2311.18654",
        "title": "detailed human-centric text description-driven large scene synthesis",
        "abstract": "text-driven large scene image synthesis has made significant progress with diffusion models, but controlling it is challenging. while using additional spatial controls with corresponding texts has improved the controllability of large scene synthesis, it is still challenging to faithfully reflect detailed text descriptions without user-provided controls. here, we propose dettext2scene, a novel text-driven large-scale image synthesis with high faithfulness, controllability, and naturalness in a global context for the detailed human-centric text description. our dettext2scene consists of 1) hierarchical keypoint-box layout generation from the detailed description by leveraging large language model (llm), 2) view-wise conditioned joint diffusion process to synthesize a large scene from the given detailed text with llm-generated grounded keypoint-box layout and 3) pixel perturbation-based pyramidal interpolation to progressively refine the large scene for global coherence. our dettext2scene significantly outperforms prior arts in text-to-large scene synthesis qualitatively and quantitatively, demonstrating strong faithfulness with detailed descriptions, superior controllability, and excellent naturalness in a global context.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18654",
        "authors": [
            "gwanghyun kim",
            "dong un kang",
            "hoigi seo",
            "hayeon kim",
            "se young chun"
        ]
    },
    {
        "id": "2311.18662",
        "title": "solving the team orienteering problem with transformers",
        "abstract": "route planning for a fleet of vehicles is an important task in applications such as package delivery, surveillance, or transportation. this problem is usually modeled as a combinatorial optimization problem named as team orienteering problem. the most popular team orienteering problem solvers are mainly based on either linear programming, which provides accurate solutions by employing a large computation time that grows with the size of the problem, or heuristic methods, which usually find suboptimal solutions in a shorter amount of time. in this paper, a multi-agent route planning system capable of solving the team orienteering problem in a very fast and accurate manner is presented. the proposed system is based on a centralized transformer neural network that can learn to encode the scenario (modeled as a graph) and the context of the agents to provide fast and accurate solutions. several experiments have been performed to demonstrate that the presented system can outperform most of the state-of-the-art works in terms of computation speed. in addition, the code is publicly available at \\url{http://gti.ssr.upm.es/data}.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18662",
        "authors": [
            "daniel fuertes",
            "carlos r. del-blanco",
            "fernando jaureguizar",
            "narciso garc\u00eda"
        ]
    },
    {
        "id": "2311.18663",
        "title": "choosing the parameter of the fermat distance: navigating geometry and   noise",
        "abstract": "the fermat distance has been recently established as a useful tool for machine learning tasks when a natural distance is not directly available to the practitioner or to improve the results given by euclidean distances by exploding the geometrical and statistical properties of the dataset. this distance depends on a parameter $\\alpha$ that greatly impacts the performance of subsequent tasks. ideally, the value of $\\alpha$ should be large enough to navigate the geometric intricacies inherent to the problem. at the same, it should remain restrained enough to sidestep any deleterious ramifications stemming from noise during the process of distance estimation. we study both theoretically and through simulations how to select this parameter.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18663",
        "authors": [
            "fr\u00e9d\u00e9ric chazal",
            "laure ferraris",
            "pablo groisman",
            "matthieu jonckheere",
            "fr\u00e9d\u00e9ric pascal",
            "facundo sapienza"
        ]
    },
    {
        "id": "2311.18664",
        "title": "multi-task learning with cross-task consistency for improved depth   estimation in colonoscopy",
        "abstract": "colonoscopy screening is the gold standard procedure for assessing abnormalities in the colon and rectum, such as ulcers and cancerous polyps. measuring the abnormal mucosal area and its 3d reconstruction can help quantify the surveyed area and objectively evaluate disease burden. however, due to the complex topology of these organs and variable physical conditions, for example, lighting, large homogeneous texture, and image modality estimating distance from the camera aka depth) is highly challenging. moreover, most colonoscopic video acquisition is monocular, making the depth estimation a non-trivial problem. while methods in computer vision for depth estimation have been proposed and advanced on natural scene datasets, the efficacy of these techniques has not been widely quantified on colonoscopy datasets. as the colonic mucosa has several low-texture regions that are not well pronounced, learning representations from an auxiliary task can improve salient feature extraction, allowing estimation of accurate camera depths. in this work, we propose to develop a novel multi-task learning (mtl) approach with a shared encoder and two decoders, namely a surface normal decoder and a depth estimator decoder. our depth estimator incorporates attention mechanisms to enhance global context awareness. we leverage the surface normal prediction to improve geometric feature extraction. also, we apply a cross-task consistency loss among the two geometrically related tasks, surface normal and camera depth. we demonstrate an improvement of 14.17% on relative error and 10.4% improvement on $\\delta_{1}$ accuracy over the most accurate baseline state-of-the-art bts approach. all experiments are conducted on a recently released c3vd dataset; thus, we provide a first benchmark of state-of-the-art methods.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18664",
        "authors": [
            "pedro esteban chavarrias solano",
            "andrew bulpitt",
            "venkataraman subramanian",
            "sharib ali"
        ]
    },
    {
        "id": "2311.18676",
        "title": "dqssa: a quantum-inspired solution for maximizing influence in online   social networks (student abstract)",
        "abstract": "influence maximization is the task of selecting optimal nodes maximising the influence spread in social networks. this study proposes a discretized quantum-based salp swarm algorithm (dqssa) for optimizing influence diffusion in social networks. by discretizing meta-heuristic algorithms and infusing them with quantum-inspired enhancements, we address issues like premature convergence and low efficacy. the proposed method, guided by quantum principles, offers a promising solution for influence maximisation. experiments on four real-world datasets reveal dqssa's superior performance as compared to established cutting-edge algorithms.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18676",
        "authors": [
            "aryaman rao",
            "parth singh",
            "dinesh kumar vishwakarma",
            "mukesh prasad"
        ]
    },
    {
        "id": "2311.18702",
        "title": "critiquellm: scaling llm-as-critic for effective and explainable   evaluation of large language model generation",
        "abstract": "since the natural language processing (nlp) community started to make large language models (llms), such as gpt-4, act as a critic to evaluate the quality of generated texts, most of them only train a critique generation model of a specific scale on specific datasets. we argue that a comprehensive investigation on the key factor of llm-based evaluation models, such as scaling properties, is lacking, so that it is still inconclusive whether these models have potential to replace gpt-4's evaluation in practical scenarios. in this paper, we propose a new critique generation model called critiquellm, which includes a dialogue-based prompting method for high-quality referenced / reference-free evaluation data. experimental results show that our model can achieve comparable evaluation performance to gpt-4 especially in system-level correlations, and even outperform gpt-4 in 3 out of 8 tasks in a challenging reference-free setting. we conduct detailed analysis to show promising scaling properties of our model in the quality of generated critiques. we also demonstrate that our generated critiques can act as scalable feedback to directly improve the generation quality of llms.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18702",
        "authors": [
            "pei ke",
            "bosi wen",
            "zhuoer feng",
            "xiao liu",
            "xuanyu lei",
            "jiale cheng",
            "shengyuan wang",
            "aohan zeng",
            "yuxiao dong",
            "hongning wang",
            "jie tang",
            "minlie huang"
        ]
    },
    {
        "id": "2311.18703",
        "title": "predictable reinforcement learning dynamics through entropy rate   minimization",
        "abstract": "in reinforcement learning (rl), agents have no incentive to exhibit predictable behaviors, and are often pushed (through e.g. policy entropy regularization) to randomize their actions in favor of exploration. from a human perspective, this makes rl agents hard to interpret and predict, and from a safety perspective, even harder to formally verify. we propose a novel method to induce predictable behavior in rl agents, referred to as predictability-aware rl (pa-rl), which employs the state sequence entropy rate as a predictability measure. we show how the entropy rate can be formulated as an average reward objective, and since its entropy reward function is policy-dependent, we introduce an action-dependent surrogate entropy enabling the use of pg methods. we prove that deterministic policies minimizing the average surrogate reward exist and also minimize the actual entropy rate, and show how, given a learned dynamical model, we are able to approximate the value function associated to the true entropy rate. finally, we demonstrate the effectiveness of the approach in rl tasks inspired by human-robot use-cases, and show how it produces agents with more predictable behavior while achieving near-optimal rewards.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18703",
        "authors": [
            "daniel jarne ornia",
            "giannis delimpaltadakis",
            "jens kober",
            "javier alonso-mora"
        ]
    },
    {
        "id": "2311.18736",
        "title": "controlgym: large-scale safety-critical control environments for   benchmarking reinforcement learning algorithms",
        "abstract": "we introduce controlgym, a library of thirty-six safety-critical industrial control settings, and ten infinite-dimensional partial differential equation (pde)-based control problems. integrated within the openai gym/gymnasium (gym) framework, controlgym allows direct applications of standard reinforcement learning (rl) algorithms like stable-baselines3. our control environments complement those in gym with continuous, unbounded action and observation spaces, motivated by real-world control applications. moreover, the pde control environments uniquely allow the users to extend the state dimensionality of the system to infinity while preserving the intrinsic dynamics. this feature is crucial for evaluating the scalability of rl algorithms for control. this project serves the learning for dynamics & control (l4dc) community, aiming to explore key questions: the convergence of rl algorithms in learning control policies; the stability and robustness issues of learning-based controllers; and the scalability of rl algorithms to high- and potentially infinite-dimensional systems. we open-source the controlgym project at https://github.com/xiangyuan-zhang/controlgym.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18736",
        "authors": [
            "xiangyuan zhang",
            "weichao mao",
            "saviz mowlavi",
            "mouhacine benosman",
            "tamer ba\u015far"
        ]
    },
    {
        "id": "2311.18741",
        "title": "vrem-fl: mobility-aware computation-scheduling co-design for vehicular   federated learning",
        "abstract": "assisted and autonomous driving are rapidly gaining momentum, and will soon become a reality. among their key enablers, artificial intelligence and machine learning are expected to play a prominent role, also thanks to the massive amount of data that smart vehicles will collect from their onboard sensors. in this domain, federated learning is one of the most effective and promising techniques for training global machine learning models, while preserving data privacy at the vehicles and optimizing communications resource usage. in this work, we propose vrem-fl, a computation-scheduling co-design for vehicular federated learning that leverages mobility of vehicles in conjunction with estimated 5g radio environment maps. vrem-fl jointly optimizes the global model learned at the server while wisely allocating communication resources. this is achieved by orchestrating local computations at the vehicles in conjunction with the transmission of their local model updates in an adaptive and predictive fashion, by exploiting radio channel maps. the proposed algorithm can be tuned to trade model training time for radio resource usage. experimental results demonstrate the efficacy of utilizing radio maps. vrem-fl outperforms literature benchmarks for both a linear regression model (learning time reduced by 28%) and a deep neural network for a semantic image segmentation task (doubling the number of model updates within the same time window).",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18741",
        "authors": [
            "luca ballotta",
            "nicol\u00f2 dal fabbro",
            "giovanni perin",
            "luca schenato",
            "michele rossi",
            "giuseppe piro"
        ]
    },
    {
        "id": "2311.18743",
        "title": "alignbench: benchmarking chinese alignment of large language models",
        "abstract": "alignment has become a critical step for instruction-tuned large language models (llms) to become helpful assistants. however, effective evaluation of alignment for emerging chinese llms is still significantly lacking, calling for real-scenario grounded, open-ended, challenging and automatic evaluations tailored for alignment. to fill in this gap, we introduce alignbench, a comprehensive multi-dimensional benchmark for evaluating llms' alignment in chinese. equipped with a human-in-the-loop data curation pipeline, our benchmark employs a rule-calibrated multi-dimensional llm-as-judge with chain-of-thought to generate explanations and final ratings as evaluations, ensuring high reliability and interpretability. furthermore, we developed a dedicated companion evaluator llm -- critiquellm, which recovers 95\\% of gpt-4's evaluation ability and will be provided via public apis to researchers for evaluation of alignment in chinese llms. all evaluation codes, data, and llm generations are available at \\url{https://github.com/thudm/alignbench}.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18743",
        "authors": [
            "xiao liu",
            "xuanyu lei",
            "shengyuan wang",
            "yue huang",
            "zhuoer feng",
            "bosi wen",
            "jiale cheng",
            "pei ke",
            "yifan xu",
            "weng lam tam",
            "xiaohan zhang",
            "lichao sun",
            "hongning wang",
            "jing zhang",
            "minlie huang",
            "yuxiao dong",
            "jie tang"
        ]
    },
    {
        "id": "2311.18749",
        "title": "transcoralnet: a two-stream transformer coral networks for supply chain   credit assessment cold start",
        "abstract": "this paper proposes an interpretable two-stream transformer coral networks (transcoralnet) for supply chain credit assessment under the segment industry and cold start problem. the model aims to provide accurate credit assessment prediction for new supply chain borrowers with limited historical data. here, the two-stream domain adaptation architecture with correlation alignment (coral) loss is used as a core model and is equipped with transformer, which provides insights about the learned features and allow efficient parallelization during training. thanks to the domain adaptation capability of the proposed model, the domain shift between the source and target domain is minimized. therefore, the model exhibits good generalization where the source and target do not follow the same distribution, and a limited amount of target labeled instances exist. furthermore, we employ local interpretable model-agnostic explanations (lime) to provide more insight into the model prediction and identify the key features contributing to supply chain credit assessment decisions. the proposed model addresses four significant supply chain credit assessment challenges: domain shift, cold start, imbalanced-class and interpretability. experimental results on a real-world data set demonstrate the superiority of transcoralnet over a number of state-of-the-art baselines in terms of accuracy. the code is available on github https://github.com/jiejieniu/transcoraln .",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18749",
        "authors": [
            "jie shi",
            "arno p. j. m. siebes",
            "siamak mehrkanoon"
        ]
    },
    {
        "id": "2311.18751",
        "title": "language model agents suffer from compositional generalization in web   automation",
        "abstract": "language model agents (lma) recently emerged as a promising paradigm on muti-step decision making tasks, often outperforming humans and other reinforcement learning agents. despite the promise, their performance on real-world applications that often involve combinations of tasks is still underexplored. in this work, we introduce a new benchmark, called compwob -- 50 new compositional web automation tasks reflecting more realistic assumptions. we show that while existing prompted lmas (gpt-3.5-turbo or gpt-4) achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks. on the other hand, transferred lmas (finetuned only on base tasks) show less generalization gap, dropping from 85.4% to 54.8%. by balancing data distribution across tasks, we train a new model, html-t5++, that surpasses human-level performance (95.2%) on miniwob, and achieves the best zero-shot performance on compwob (61.5%). while these highlight the promise of small-scale finetuned and transferred models for compositional generalization, their performance further degrades under different instruction compositions changing combinational order. in contrast to the recent remarkable success of lma, our benchmark and detailed analysis emphasize the necessity of building lmas that are robust and generalizable to task compositionality for real-world deployment.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18751",
        "authors": [
            "hiroki furuta",
            "yutaka matsuo",
            "aleksandra faust",
            "izzeddin gur"
        ]
    },
    {
        "id": "2311.18760",
        "title": "taskbench: benchmarking large language models for task automation",
        "abstract": "recently, the incredible progress of large language models (llms) has ignited the spark of task automation, which decomposes the complex tasks described by user instructions into sub-tasks, and invokes external tools to execute them, and plays a central role in autonomous agents. however, there lacks a systematic and standardized benchmark to foster the development of llms in task automation. to this end, we introduce taskbench to evaluate the capability of llms in task automation. specifically, task automation can be formulated into three critical stages: task decomposition, tool invocation, and parameter prediction to fulfill user intent. this complexity makes data collection and evaluation more challenging compared to common nlp tasks. to generate high-quality evaluation datasets, we introduce the concept of tool graph to represent the decomposed tasks in user intent, and adopt a back-instruct method to simulate user instruction and annotations. furthermore, we propose taskeval to evaluate the capability of llms from different aspects, including task decomposition, tool invocation, and parameter prediction. experimental results demonstrate that taskbench can effectively reflects the capability of llms in task automation. benefiting from the mixture of automated data construction and human verification, taskbench achieves a high consistency compared to the human evaluation, which can be utilized as a comprehensive and faithful benchmark for llm-based autonomous agents.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18760",
        "authors": [
            "yongliang shen",
            "kaitao song",
            "xu tan",
            "wenqi zhang",
            "kan ren",
            "siyu yuan",
            "weiming lu",
            "dongsheng li",
            "yueting zhuang"
        ]
    },
    {
        "id": "2311.18763",
        "title": "continual diffusion with stamina: stack-and-mask incremental adapters",
        "abstract": "recent work has demonstrated a remarkable ability to customize text-to-image diffusion models to multiple, fine-grained concepts in a sequential (i.e., continual) manner while only providing a few example images for each concept. this setting is known as continual diffusion. here, we ask the question: can we scale these methods to longer concept sequences without forgetting? although prior work mitigates the forgetting of previously learned concepts, we show that its capacity to learn new tasks reaches saturation over longer sequences. we address this challenge by introducing a novel method, stack-and-mask incremental adapters (stamina), which is composed of low-ranked attention-masked adapters and customized mlp tokens. stamina is designed to enhance the robust fine-tuning properties of lora for sequential concept learning via learnable hard-attention masks parameterized with low rank mlps, enabling precise, scalable learning via sparse adaptation. notably, all introduced trainable parameters can be folded back into the model after training, inducing no additional inference parameter costs. we show that stamina outperforms the prior sota for the setting of text-to-image continual customization on a 50-concept benchmark composed of landmarks and human faces, with no stored replay data. additionally, we extended our method to the setting of continual learning for image classification, demonstrating that our gains also translate to state-of-the-art performance in this standard benchmark.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18763",
        "authors": [
            "james seale smith",
            "yen-chang hsu",
            "zsolt kira",
            "yilin shen",
            "hongxia jin"
        ]
    },
    {
        "id": "2311.18765",
        "title": "mllms-augmented visual-language representation learning",
        "abstract": "visual-language pre-training (vlp) have achieved remarkable success in multi-modal tasks, largely attributed to the availability of large-scale image-text datasets. in this work, we demonstrate that multi-modal large language models (mllms) can enhance visual-language representation learning by improving data quality. our approach is simple, utilizing mllms to extend multiple captions for each image. to prevent the bias that introduced by mllms' hallucinations and intrinsic caption styles, we propose a \"text shearing\" to keep the lengths of extended captions identical to the originals. in image-text retrieval, our method consistently obtains 5.6 ~ 35.0% and 16.8 ~ 46.1% improvement on r@1 under the fine-tuning and zero-shot settings, respectively. notably, our zero-shot results are comparable to fine-tuning on target datasets, which encourages more exploration on the versatile use of mllms.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18765",
        "authors": [
            "yanqing liu",
            "kai wang",
            "wenqi shao",
            "ping luo",
            "yu qiao",
            "mike zheng shou",
            "kaipeng zhang",
            "yang you"
        ]
    },
    {
        "id": "2311.18768",
        "title": "evaluating the impact of flaky simulators on testing autonomous driving   systems",
        "abstract": "simulators are widely used to test autonomous driving systems (ads), but their potential flakiness can lead to inconsistent test results. we investigate test flakiness in simulation-based testing of ads by addressing two key questions: (1) how do flaky ads simulations impact automated testing that relies on randomized algorithms? and (2) can machine learning (ml) effectively identify flaky ads tests while decreasing the required number of test reruns? our empirical results, obtained from two widely-used open-source ads simulators and five diverse ads test setups, show that test flakiness in ads is a common occurrence and can significantly impact the test results obtained by randomized algorithms. further, our ml classifiers effectively identify flaky ads tests using only a single test run, achieving f1-scores of $85$%, $82$% and $96$% for three different ads test setups. our classifiers significantly outperform our non-ml baseline, which requires executing tests at least twice, by $31$%, $21$%, and $13$% in f1-score performance, respectively. we conclude with a discussion on the scope, implications and limitations of our study. we provide our complete replication package in a github repository.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18768",
        "authors": [
            "mohammad hossein amini",
            "shervin naseri",
            "shiva nejati"
        ]
    },
    {
        "id": "2311.18775",
        "title": "codi-2: in-context, interleaved, and interactive any-to-any generation",
        "abstract": "we present codi-2, a versatile and interactive multimodal large language model (mllm) that can follow complex multimodal interleaved instructions, conduct in-context learning (icl), reason, chat, edit, etc., in an any-to-any input-output modality paradigm. by aligning modalities with language for both encoding and generation, codi-2 empowers large language models (llms) to not only understand complex modality-interleaved instructions and in-context examples, but also autoregressively generate grounded and coherent multimodal outputs in the continuous feature space. to train codi-2, we build a large-scale generation dataset encompassing in-context multimodal instructions across text, vision, and audio. codi-2 demonstrates a wide range of zero-shot capabilities for multimodal generation, such as in-context learning, reasoning, and compositionality of any-to-any modality generation through multi-round interactive conversation. codi-2 surpasses previous domain-specific models on tasks such as subject-driven image generation, vision transformation, and audio editing. codi-2 signifies a substantial breakthrough in developing a comprehensive multimodal foundation model adept at interpreting in-context language-vision-audio interleaved instructions and producing multimodal outputs.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18775",
        "authors": [
            "zineng tang",
            "ziyi yang",
            "mahmoud khademi",
            "yang liu",
            "chenguang zhu",
            "mohit bansal"
        ]
    },
    {
        "id": "2311.18788",
        "title": "automated interpretation of congenital heart disease from multi-view   echocardiograms",
        "abstract": "congenital heart disease (chd) is the most common birth defect and the leading cause of neonate death in china. clinical diagnosis can be based on the selected 2d key-frames from five views. limited by the availability of multi-view data, most methods have to rely on the insufficient single view analysis. this study proposes to automatically analyze the multi-view echocardiograms with a practical end-to-end framework. we collect the five-view echocardiograms video records of 1308 subjects (including normal controls, ventricular septal defect (vsd) patients and atrial septal defect (asd) patients) with both disease labels and standard-view key-frame labels. depthwise separable convolution-based multi-channel networks are adopted to largely reduce the network parameters. we also approach the imbalanced class problem by augmenting the positive training samples. our 2d key-frame model can diagnose chd or negative samples with an accuracy of 95.4\\%, and in negative, vsd or asd classification with an accuracy of 92.3\\%. to further alleviate the work of key-frame selection in real-world implementation, we propose an adaptive soft attention scheme to directly explore the raw video data. four kinds of neural aggregation methods are systematically investigated to fuse the information of an arbitrary number of frames in a video. moreover, with a view detection module, the system can work without the view records. our video-based model can diagnose with an accuracy of 93.9\\% (binary classification), and 92.1\\% (3-class classification) in a collected 2d video testing set, which does not need key-frame selection and view annotation in testing. the detailed ablation study and the interpretability analysis are provided.",
        "doi": "10.1016/j.media.2020.101942",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18788",
        "authors": [
            "jing wang",
            "xiaofeng liu",
            "fangyun wang",
            "lin zheng",
            "fengqiao gao",
            "hanwen zhang",
            "xin zhang",
            "wanqing xie",
            "binbin wang"
        ]
    },
    {
        "id": "2311.18801",
        "title": "distributed global structure-from-motion with a deep front-end",
        "abstract": "while initial approaches to structure-from-motion (sfm) revolved around both global and incremental methods, most recent applications rely on incremental systems to estimate camera poses due to their superior robustness. though there has been tremendous progress in sfm `front-ends' powered by deep models learned from data, the state-of-the-art (incremental) sfm pipelines still rely on classical sift features, developed in 2004. in this work, we investigate whether leveraging the developments in feature extraction and matching helps global sfm perform on par with the sota incremental sfm approach (colmap). to do so, we design a modular sfm framework that allows us to easily combine developments in different stages of the sfm pipeline. our experiments show that while developments in deep-learning based two-view correspondence estimation do translate to improvements in point density for scenes reconstructed with global sfm, none of them outperform sift when comparing with incremental sfm results on a range of datasets. our sfm system is designed from the ground up to leverage distributed computation, enabling us to parallelize computation on multiple machines and scale to large scenes.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18801",
        "authors": [
            "ayush baid",
            "john lambert",
            "travis driver",
            "akshay krishnan",
            "hayk stepanyan",
            "frank dellaert"
        ]
    },
    {
        "id": "2311.18805",
        "title": "unnatural error correction: gpt-4 can almost perfectly handle unnatural   scrambled text",
        "abstract": "while large language models (llms) have achieved remarkable performance in many tasks, much about their inner workings remains unclear. in this study, we present novel experimental insights into the resilience of llms, particularly gpt-4, when subjected to extensive character-level permutations. to investigate this, we first propose the scrambled bench, a suite designed to measure the capacity of llms to handle scrambled input, in terms of both recovering scrambled sentences and answering questions given scrambled context. the experimental results indicate that most powerful llms demonstrate the capability akin to typoglycemia, a phenomenon where humans can understand the meaning of words even when the letters within those words are scrambled, as long as the first and last letters remain in place. more surprisingly, we found that only gpt-4 nearly flawlessly processes inputs with unnatural errors, even under the extreme condition, a task that poses significant challenges for other llms and often even for humans. specifically, gpt-4 can almost perfectly reconstruct the original sentences from scrambled ones, decreasing the edit distance by 95%, even when all letters within each word are entirely scrambled. it is counter-intuitive that llms can exhibit such resilience despite severe disruption to input tokenization caused by scrambled text.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18805",
        "authors": [
            "qi cao",
            "takeshi kojima",
            "yutaka matsuo",
            "yusuke iwasawa"
        ]
    },
    {
        "id": "2311.18817",
        "title": "dichotomy of early and late phase implicit biases can provably induce   grokking",
        "abstract": "recent work by power et al. (2022) highlighted a surprising \"grokking\" phenomenon in learning arithmetic tasks: a neural net first \"memorizes\" the training set, resulting in perfect training accuracy but near-random test accuracy, and after training for sufficiently longer, it suddenly transitions to perfect test accuracy. this paper studies the grokking phenomenon in theoretical setups and shows that it can be induced by a dichotomy of early and late phase implicit biases. specifically, when training homogeneous neural nets with large initialization and small weight decay on both classification and regression tasks, we prove that the training process gets trapped at a solution corresponding to a kernel predictor for a long time, and then a very sharp transition to min-norm/max-margin predictors occurs, leading to a dramatic change in test accuracy.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18817",
        "authors": [
            "kaifeng lyu",
            "jikai jin",
            "zhiyuan li",
            "simon s. du",
            "jason d. lee",
            "wei hu"
        ]
    },
    {
        "id": "2311.18827",
        "title": "motion-conditioned image animation for video editing",
        "abstract": "we introduce moca, a motion-conditioned image animation approach for video editing. it leverages a simple decomposition of the video editing problem into image editing followed by motion-conditioned image animation. furthermore, given the lack of robust evaluation datasets for video editing, we introduce a new benchmark that measures edit capability across a wide variety of tasks, such as object replacement, background changes, style changes, and motion edits. we present a comprehensive human evaluation of the latest video editing methods along with moca, on our proposed benchmark. moca establishes a new state-of-the-art, demonstrating greater human preference win-rate, and outperforming notable recent approaches including dreamix (63%), masactrl (75%), and tune-a-video (72%), with especially significant improvements for motion edits.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18827",
        "authors": [
            "wilson yan",
            "andrew brown",
            "pieter abbeel",
            "rohit girdhar",
            "samaneh azadi"
        ]
    },
    {
        "id": "2311.18837",
        "title": "vidiff: translating videos via multi-modal instructions with diffusion   models",
        "abstract": "diffusion models have achieved significant success in image and video generation. this motivates a growing interest in video editing tasks, where videos are edited according to provided text descriptions. however, most existing approaches only focus on video editing for short clips and rely on time-consuming tuning or inference. we are the first to propose video instruction diffusion (vidiff), a unified foundation model designed for a wide range of video tasks. these tasks encompass both understanding tasks (such as language-guided video object segmentation) and generative tasks (video editing and enhancement). our model can edit and translate the desired results within seconds based on user instructions. moreover, we design an iterative auto-regressive method to ensure consistency in editing and enhancing long videos. we provide convincing generative results for diverse input videos and written instructions, both qualitatively and quantitatively. more examples can be found at our website https://chenhsing.github.io/vidiff.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18837",
        "authors": [
            "zhen xing",
            "qi dai",
            "zihao zhang",
            "hui zhang",
            "han hu",
            "zuxuan wu",
            "yu-gang jiang"
        ]
    },
    {
        "id": "2311.18838",
        "title": "dataset distillation in large data era",
        "abstract": "dataset distillation aims to generate a smaller but representative subset from a large dataset, which allows a model to be trained efficiently, meanwhile evaluating on the original testing data distribution to achieve decent performance. many prior works have aimed to align with diverse aspects of the original datasets, such as matching the training weight trajectories, gradient, feature/batchnorm distributions, etc. in this work, we show how to distill various large-scale datasets such as full imagenet-1k/21k under a conventional input resolution of 224$\\times$224 to achieve the best accuracy over all previous approaches, including sre$^2$l, tesla and mtt. to achieve this, we introduce a simple yet effective ${\\bf c}$urriculum ${\\bf d}$ata ${\\bf a}$ugmentation ($\\texttt{cda}$) during data synthesis that obtains the accuracy on large-scale imagenet-1k and 21k with 63.2% under ipc (images per class) 50 and 36.1% under ipc 20, respectively. finally, we show that, by integrating all our enhancements together, the proposed model beats the current state-of-the-art by more than 4% top-1 accuracy on imagenet-1k/21k and for the first time, reduces the gap to its full-data training counterpart to less than absolute 15%. moreover, this work represents the inaugural success in dataset distillation on larger-scale imagenet-21k under the standard 224$\\times$224 resolution. our code and distilled imagenet-21k dataset of 20 ipc, 2k recovery budget are available at https://github.com/vila-lab/sre2l/tree/main/cda.",
        "doi": "",
        "created": "2023-11-30",
        "url": "https://arxiv.org/abs/2311.18838",
        "authors": [
            "zeyuan yin",
            "zhiqiang shen"
        ]
    }
]